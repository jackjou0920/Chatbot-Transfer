{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "streaming-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from transformers import AdamW, BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "after-diversity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OTHER': 0, 'TRANSFER': 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_vals = ['OTHER', 'TRANSFER']\n",
    "tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "strong-crime",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>幫忙從台幣帳戶轉3560塊到父親的帳戶</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>幫轉8028到我的父親帳戶從台幣帳戶</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>我要轉5890從我薪轉戶到父親</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>要轉去我的父親戶頭8711塊從我台幣戶</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>請幫忙轉去我的父親從我的外幣戶696塊錢</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108740</th>\n",
       "      <td>人們願意與他做生意有時商業事務通過電話即可辦理</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108741</th>\n",
       "      <td>經過十幾年的努力他已成為世界最大的私人集裝箱船船主</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108742</th>\n",
       "      <td>妻賢子孝家庭幸福</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108743</th>\n",
       "      <td>希臘人將瓦西里斯與奧納西斯比較時總不忘補充一句他和奧納西斯不同他沒有改組家庭</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108744</th>\n",
       "      <td>重視傳統家庭觀念的希臘人對瓦西里斯幸福的家庭充滿讚譽</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108745 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text  label\n",
       "0                          幫忙從台幣帳戶轉3560塊到父親的帳戶      1\n",
       "1                           幫轉8028到我的父親帳戶從台幣帳戶      1\n",
       "2                              我要轉5890從我薪轉戶到父親      1\n",
       "3                          要轉去我的父親戶頭8711塊從我台幣戶      1\n",
       "4                         請幫忙轉去我的父親從我的外幣戶696塊錢      1\n",
       "...                                        ...    ...\n",
       "108740                 人們願意與他做生意有時商業事務通過電話即可辦理      0\n",
       "108741               經過十幾年的努力他已成為世界最大的私人集裝箱船船主      0\n",
       "108742                                妻賢子孝家庭幸福      0\n",
       "108743  希臘人將瓦西里斯與奧納西斯比較時總不忘補充一句他和奧納西斯不同他沒有改組家庭      0\n",
       "108744              重視傳統家庭觀念的希臘人對瓦西里斯幸福的家庭充滿讚譽      0\n",
       "\n",
       "[108745 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset_classification.csv\")\n",
    "df.columns = ['text', 'label']\n",
    "df[\"label\"] = df[\"label\"].apply(lambda x: tag2idx[x])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mysterious-reviewer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 3060'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ordinary-going",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-chinese\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "# tokenizer = BertTokenizer(\"bert-base-chinese-vocab.txt\")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "instructional-anthropology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最長輸入長度: 70\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 0\n",
    "for sent in df['text'].values.tolist():\n",
    "    if(len(sent) > MAX_LEN):\n",
    "        MAX_LEN = len(sent)\n",
    "    \n",
    "print(\"最長輸入長度:\", MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "medium-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "spoken-motel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.dataset = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset.loc[idx, \"text\"]\n",
    "        label = self.dataset.loc[idx, \"label\"]\n",
    "        sample = {\"text\": text, \"label\": label}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "labeled-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.sample(frac=1).reset_index(drop=True)\n",
    "# df_train = df.head(int(0.8*len(df)))\n",
    "# df_valid = df.tail(int(0.2*len(df)))\n",
    "# len(df_train), len(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "earlier-greek",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TransferDataset(df)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "validset = TransferDataset(df)\n",
    "validloader = DataLoader(validset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "outdoor-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_ids(tokenizer, text, max_len=70):\n",
    "    if isinstance(text, str):\n",
    "        tokenized_text = tokenizer.encode_plus(text, max_length=max_len, add_special_tokens=True)\n",
    "        input_ids = tokenized_text[\"input_ids\"]\n",
    "        token_type_ids = tokenized_text[\"token_type_ids\"]\n",
    "    elif isinstance(text, list):\n",
    "        input_ids = []\n",
    "        token_type_ids = []\n",
    "        for t in text:\n",
    "            tokenized_text = tokenizer.encode_plus(t, max_length=max_len, add_special_tokens=True)\n",
    "            input_ids.append(tokenized_text[\"input_ids\"])\n",
    "            token_type_ids.append(tokenized_text[\"token_type_ids\"])\n",
    "    else:\n",
    "        print(\"Unexpected input\")\n",
    "    return input_ids, token_type_ids\n",
    "\n",
    "def seq_padding(tokenizer, X):\n",
    "    pad_id = tokenizer.convert_tokens_to_ids(\"[PAD]\")\n",
    "    if len(X) <= 1:\n",
    "        return torch.tensor(X)\n",
    "    L = [len(x) for x in X]\n",
    "    ML = max(L)\n",
    "    X = torch.Tensor([x + [pad_id] * (ML - len(x)) if len(x) < ML else x for x in X])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "seeing-nickel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "        {\n",
    "            'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            'weight_decay': 1e-2\n",
    "        }, {\n",
    "            'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            'weight_decay': 0.0\n",
    "        }\n",
    "]\n",
    "\n",
    "#optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "prepared-senator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        text = batch[\"text\"]\n",
    "        label = batch[\"label\"]\n",
    "        \n",
    "        input_ids, token_type_ids = convert_text_to_ids(tokenizer, text, MAX_LEN)\n",
    "        input_ids = seq_padding(tokenizer, input_ids)\n",
    "        token_type_ids = seq_padding(tokenizer, token_type_ids)\n",
    "        \n",
    "        label = label.unsqueeze(1)  # (batch_size, 1)\n",
    "        input_ids, token_type_ids, label = input_ids.long(), token_type_ids.long(), label.long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        input_ids, token_type_ids, label = input_ids.to(device), token_type_ids.to(device), label.to(device)\n",
    "        output = model(input_ids=input_ids, token_type_ids=token_type_ids, labels=label)\n",
    "    \n",
    "        y_pred_prob = output[1]\n",
    "        y_pred_label = y_pred_prob.argmax(dim=1)\n",
    "        \n",
    "        # 计算loss, 这个 loss 和 output[0] 是一样的\n",
    "        loss = criterion(y_pred_prob.view(-1, 2), label.view(-1))\n",
    "        #loss = output[0]\n",
    "        acc = ((y_pred_label == label.view(-1)).sum()).item()\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "        \n",
    "        if i % 200 == 0:\n",
    "            print(\"current loss:\", epoch_loss / (i+1), \"\\t\", \"current acc:\", epoch_acc / ((i+1)*len(label)))\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator.dataset.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "noted-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(iterator):\n",
    "            text = batch[\"text\"]\n",
    "            label = batch[\"label\"]\n",
    "            \n",
    "            input_ids, token_type_ids = convert_text_to_ids(tokenizer, text, MAX_LEN)\n",
    "            input_ids = seq_padding(tokenizer, input_ids)\n",
    "            token_type_ids = seq_padding(tokenizer, token_type_ids)\n",
    "\n",
    "            label = label.unsqueeze(1)\n",
    "            input_ids, token_type_ids, label = input_ids.long(), token_type_ids.long(), label.long()\n",
    "            input_ids, token_type_ids, label = input_ids.to(device), token_type_ids.to(device), label.to(device)\n",
    "\n",
    "            output = model(input_ids=input_ids, token_type_ids=token_type_ids, labels=label)\n",
    "            y_pred_label = output[1].argmax(dim=1)\n",
    "            \n",
    "            loss = output[0]\n",
    "            acc = ((y_pred_label == label.view(-1)).sum()).item()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator.dataset.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "central-payroll",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current loss: 0.7969380617141724 \t current acc: 0.3125\n",
      "current loss: 0.08219996878740596 \t current acc: 0.974502487562189\n",
      "current loss: 0.043424009123382776 \t current acc: 0.9865960099750624\n",
      "current loss: 0.029627097342682446 \t current acc: 0.9908485856905158\n",
      "current loss: 0.022333447182353076 \t current acc: 0.9931335830212235\n",
      "current loss: 0.017918927688291158 \t current acc: 0.9945054945054945\n",
      "current loss: 0.01575971455946231 \t current acc: 0.9952643630308077\n",
      "current loss: 0.01353629006124896 \t current acc: 0.9959403997144897\n",
      "current loss: 0.011861007136140347 \t current acc: 0.996447532792005\n",
      "current loss: 0.010554910104166355 \t current acc: 0.9968420322043309\n",
      "current loss: 0.00951490300060217 \t current acc: 0.9971576711644178\n",
      "current loss: 0.0086634630347926 \t current acc: 0.9974159472966834\n",
      "current loss: 0.007946293594290848 \t current acc: 0.997631195335277\n",
      "current loss: 0.007338750511073343 \t current acc: 0.9978133410226836\n",
      "current loss: 0.006817544473651112 \t current acc: 0.9979694751874331\n",
      "current loss: 0.006365522926242108 \t current acc: 0.9981047984005331\n",
      "current loss: 0.0059696768135476756 \t current acc: 0.9982232114964074\n",
      "current loss: 0.005620278282696755 \t current acc: 0.99832769773596\n",
      "current loss: 0.005309486728841803 \t current acc: 0.9984205776173285\n",
      "current loss: 0.005031274123223401 \t current acc: 0.9985036832412523\n",
      "current loss: 0.004780743404718373 \t current acc: 0.998578480379905\n",
      "current loss: 0.004554008482901056 \t current acc: 0.9986461556772197\n",
      "current loss: 0.00434778526045566 \t current acc: 0.9987076800727107\n",
      "current loss: 0.004288000800939077 \t current acc: 0.9987231036731146\n",
      "current loss: 0.00423954185249305 \t current acc: 0.9987502603624245\n",
      "current loss: 0.004071331429295235 \t current acc: 0.9988002399520096\n",
      "current loss: 0.003915780679320867 \t current acc: 0.9988463756969813\n",
      "current loss: 0.003771979837211945 \t current acc: 0.9988890946121088\n",
      "current loss: 0.0036380193558045804 \t current acc: 0.9989287627209427\n",
      "current loss: 0.003513245309055596 \t current acc: 0.9989656955697294\n",
      "current loss: 0.0033966747864168853 \t current acc: 0.9990001666388936\n",
      "current loss: 0.0033219632067413405 \t current acc: 0.9990223351072408\n",
      "current loss: 0.0032200398788326825 \t current acc: 0.999052882362131\n",
      "current loss: 0.0031364245649854743 \t current acc: 0.9990721102863203\n",
      "train loss:  0.003173315518947147 \t train acc: 0.9990804174904593\n",
      "valid loss:  7.461760849169775e-05 \t valid acc: 0.9999816083498092\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "for i in range(epochs):\n",
    "    train_loss, train_acc = train(model, trainloader, optimizer, criterion, device)\n",
    "    print(\"train loss: \", train_loss, \"\\t\", \"train acc:\", train_acc)\n",
    "    \n",
    "    valid_loss, valid_acc = evaluate(model, validloader, criterion, device)\n",
    "    print(\"valid loss: \", valid_loss, \"\\t\", \"valid acc:\", valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "identical-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model_classification_gpu_epoch_\"+str(epochs)+\"_batch_\"+str(batch_size))\n",
    "torch.save(model.module,\"model_classification_gpu_epoch_\"+str(epochs)+\"_batch_\"+str(batch_size)+\".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-patient",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-italian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "retained-appointment",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"model_classification_gpu_epoch_1_batch_64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "australian-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_classification(text):\n",
    "    input_ids, token_type_ids = convert_text_to_ids(tokenizer, text)\n",
    "    input_ids = seq_padding(tokenizer, input_ids)\n",
    "    token_type_ids = seq_padding(tokenizer, token_type_ids)\n",
    "\n",
    "    input_ids, token_type_ids = input_ids.long(), token_type_ids.long()\n",
    "    input_ids, token_type_ids = input_ids.to(device), token_type_ids.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "\n",
    "    logits = output.logits.detach().cpu().numpy()\n",
    "    return(np.argmax(logits, axis=1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "amended-productivity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "text = ['我想轉給父親的戶頭5063元從薪轉']\n",
    "result = predict_classification(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-switzerland",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
