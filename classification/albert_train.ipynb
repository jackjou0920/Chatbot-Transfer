{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "weird-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "junior-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "opposite-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_model(model_name, config_file_path, model_file_path, vocab_file_path, num_labels):\n",
    "    # 選擇模型並加載設定\n",
    "    if(model_name == 'bert'):\n",
    "        from transformers import BertConfig, BertForSequenceClassification, BertTokenizer\n",
    "        \n",
    "        model_config, model_class, model_tokenizer = (BertConfig, BertForSequenceClassification, BertTokenizer)\n",
    "        config = model_config.from_pretrained(config_file_path,num_labels = num_labels)\n",
    "        model = model_class.from_pretrained(model_file_path, from_tf=bool('.ckpt' in 'bert-base-chinese'), config=config)\n",
    "        tokenizer = model_tokenizer(vocab_file=vocab_file_path)\n",
    "        return model, tokenizer\n",
    "    \n",
    "    elif(model_name == 'albert'):\n",
    "        from albert.albert_zh import AlbertConfig, AlbertTokenizer, AlbertForSequenceClassification\n",
    "        \n",
    "        model_config, model_class, model_tokenizer = (AlbertConfig, AlbertForSequenceClassification, AlbertTokenizer)\n",
    "        config = model_config.from_pretrained(config_file_path, num_labels=num_labels)\n",
    "        model = model_class.from_pretrained(model_file_path, config=config)\n",
    "        tokenizer = model_tokenizer.from_pretrained(vocab_file_path)\n",
    "        return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "regional-water",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將文字輸入轉換成對應的id編號\n",
    "def to_bert_ids(tokenizer, q_input):\n",
    "    return tokenizer.build_inputs_with_special_tokens(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(q_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "nasty-george",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(full_dataset, split_rate=0.8):  \n",
    "    train_size = int(split_rate * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pharmaceutical-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataDic(object):\n",
    "    def __init__(self, answers):\n",
    "        self.answers = answers #全部答案(含重複)\n",
    "        self.answers_norepeat = sorted(list(set(answers))) # 不重複\n",
    "        self.answers_types = len(self.answers_norepeat) # 總共多少類\n",
    "        self.ans_list = [] # 用於查找id或是text的list\n",
    "        self._make_dic() # 製作字典\n",
    "    \n",
    "    def _make_dic(self):\n",
    "        for index_a, a in enumerate(self.answers_norepeat):\n",
    "            if a != None:\n",
    "                self.ans_list.append((index_a, a))\n",
    "\n",
    "    def to_id(self, text):\n",
    "        for ans_id, ans_text in self.ans_list:\n",
    "            if text == ans_text:\n",
    "                return ans_id\n",
    "\n",
    "    def to_text(self, id):\n",
    "        for ans_id,ans_text in self.ans_list:\n",
    "            if id == ans_id:\n",
    "                return ans_text\n",
    "\n",
    "    @property\n",
    "    def types(self):\n",
    "        return self.answers_types\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        return self.answers\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "primary-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_feature(tokenizer, train_data_path):\n",
    "    df_data = pd.read_csv(train_data_path, encoding='utf-8')\n",
    "    \n",
    "    questions = df_data['sentence'].values.tolist()\n",
    "    answers = df_data['label'].values.tolist()\n",
    "    \n",
    "    print(len(questions), len(answers))\n",
    "    assert len(answers) == len(questions)\n",
    "    \n",
    "    ans_dic = DataDic(answers)\n",
    "    question_dic = DataDic(questions)\n",
    "\n",
    "    q_tokens = []\n",
    "    max_seq_len = 0\n",
    "    for q in question_dic.data:\n",
    "        bert_ids = to_bert_ids(tokenizer, q)\n",
    "        if(len(bert_ids) > max_seq_len):\n",
    "            max_seq_len = len(bert_ids)\n",
    "        q_tokens.append(bert_ids)\n",
    "        # print(tokenizer.convert_ids_to_tokens(tokenizer.build_inputs_with_special_tokens(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(q)))))\n",
    "    \n",
    "    print(\"最長輸入長度:\",max_seq_len)\n",
    "    assert max_seq_len <= 512 # 小於BERT-base長度限制\n",
    "\n",
    "    # 補齊長度\n",
    "    for q in q_tokens:\n",
    "        while len(q) < max_seq_len:\n",
    "            q.append(0)\n",
    "    \n",
    "    a_labels = []\n",
    "    for a in ans_dic.data:\n",
    "        a_labels.append(ans_dic.to_id(a))\n",
    "        # print (ans_dic.to_id(a))\n",
    "    \n",
    "    # BERT input embedding\n",
    "    answer_lables = a_labels\n",
    "    input_ids = q_tokens\n",
    "    input_masks = [[1]*max_seq_len for i in range(len(question_dic))]  # 1\n",
    "    input_segment_ids = [[0]*max_seq_len for i in range(len(question_dic))]  # 0\n",
    "    assert len(input_ids) == len(question_dic) and len(input_ids) == len(input_masks) and len(input_ids) == len(input_segment_ids)\n",
    "\n",
    "    data_features = {\n",
    "        'input_ids': input_ids,\n",
    "        'input_masks': input_masks,\n",
    "        'input_segment_ids': input_segment_ids,\n",
    "        'answer_lables': answer_lables,\n",
    "        'question_dic': question_dic,\n",
    "        'answer_dic': ans_dic\n",
    "    }\n",
    "    \n",
    "    output = open('trained_model/data_features.pkl', 'wb')\n",
    "    pickle.dump(data_features, output)\n",
    "    return data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "muslim-effects",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_setting = {\n",
    "    \"model_name\":\"bert\", \n",
    "    \"config_file_path\":\"bert-base-chinese\", \n",
    "    \"model_file_path\":\"bert-base-chinese\", \n",
    "    \"vocab_file_path\":\"bert-base-chinese-vocab.txt\",\n",
    "    \"num_labels\":2  # 分幾類 \n",
    "}\n",
    "\n",
    "# model_setting = {\n",
    "#     \"model_name\":\"albert\", \n",
    "#     \"config_file_path\":\"albert/albert_tiny/config.json\", \n",
    "#     \"model_file_path\":\"albert/albert_tiny/pytorch_model.bin\", \n",
    "#     \"vocab_file_path\":\"albert/albert_tiny/vocab.txt\",\n",
    "#     \"num_labels\":2 # 分幾類\n",
    "# }\n",
    "\n",
    "model, tokenizer = use_model(**model_setting)\n",
    "\n",
    "# setting device    \n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"using device\", device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "complete-limitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108745 108745\n",
      "最長輸入長度: 72\n"
     ]
    }
   ],
   "source": [
    "data_feature = convert_data_to_feature(tokenizer, './dataset_classification.csv')\n",
    "input_ids = data_feature['input_ids']\n",
    "input_masks = data_feature['input_masks']  # 1\n",
    "input_segment_ids = data_feature['input_segment_ids']  # 0\n",
    "answer_lables = data_feature['answer_lables']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "radio-concord",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(input_ids, input_masks, input_segment_ids, answer_lables):\n",
    "    all_input_ids = torch.tensor([input_id for input_id in input_ids], dtype=torch.long)\n",
    "    all_input_masks = torch.tensor([input_mask for input_mask in input_masks], dtype=torch.long)\n",
    "    all_input_segment_ids = torch.tensor([input_segment_id for input_segment_id in input_segment_ids], dtype=torch.long)\n",
    "    all_answer_lables = torch.tensor([answer_lable for answer_lable in answer_lables], dtype=torch.long)    \n",
    "    \n",
    "    return TensorDataset(all_input_ids, all_input_masks, all_input_segment_ids, all_answer_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "civilian-federal",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "activated-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = make_dataset(input_ids=input_ids, input_masks=input_masks, input_segment_ids=input_segment_ids, answer_lables=answer_lables)\n",
    "train_dataset, test_dataset = split_dataset(full_dataset, 0.8)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "labeled-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_target):\n",
    "    # 計算正確率\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "distinct-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=5e-6, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "shaped-scottish",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "informed-employee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch:   1 train_loss:0.6196 train_acc:70.3125\n",
      "epoch: 1 batch:   2 train_loss:0.6318 train_acc:67.1875\n",
      "epoch: 1 batch:   3 train_loss:0.6292 train_acc:66.1458\n",
      "epoch: 1 batch:   4 train_loss:0.6184 train_acc:66.7969\n",
      "epoch: 1 batch:   5 train_loss:0.6009 train_acc:69.6875\n",
      "epoch: 1 batch:   6 train_loss:0.5826 train_acc:72.9167\n",
      "epoch: 1 batch:   7 train_loss:0.5652 train_acc:75.6696\n",
      "epoch: 1 batch:   8 train_loss:0.5507 train_acc:77.9297\n",
      "epoch: 1 batch:   9 train_loss:0.5361 train_acc:80.0347\n",
      "epoch: 1 batch:  10 train_loss:0.5206 train_acc:81.7188\n",
      "epoch: 1 batch:  11 train_loss:0.5072 train_acc:82.8125\n",
      "epoch: 1 batch:  12 train_loss:0.4941 train_acc:83.8542\n",
      "epoch: 1 batch:  13 train_loss:0.4801 train_acc:85.0962\n",
      "epoch: 1 batch:  14 train_loss:0.4680 train_acc:85.7143\n",
      "epoch: 1 batch:  15 train_loss:0.4574 train_acc:86.3542\n",
      "epoch: 1 batch:  16 train_loss:0.4455 train_acc:87.1094\n",
      "epoch: 1 batch:  17 train_loss:0.4331 train_acc:87.8676\n",
      "epoch: 1 batch:  18 train_loss:0.4205 train_acc:88.5417\n",
      "epoch: 1 batch:  19 train_loss:0.4105 train_acc:89.0625\n",
      "epoch: 1 batch:  20 train_loss:0.4000 train_acc:89.5312\n",
      "epoch: 1 batch:  21 train_loss:0.3907 train_acc:89.8810\n",
      "epoch: 1 batch:  22 train_loss:0.3806 train_acc:90.3409\n",
      "epoch: 1 batch:  23 train_loss:0.3722 train_acc:90.7609\n",
      "epoch: 1 batch:  24 train_loss:0.3621 train_acc:91.1458\n",
      "epoch: 1 batch:  25 train_loss:0.3535 train_acc:91.4375\n",
      "epoch: 1 batch:  26 train_loss:0.3455 train_acc:91.7067\n",
      "epoch: 1 batch:  27 train_loss:0.3376 train_acc:91.9560\n",
      "epoch: 1 batch:  28 train_loss:0.3293 train_acc:92.2433\n",
      "epoch: 1 batch:  29 train_loss:0.3226 train_acc:92.4569\n",
      "epoch: 1 batch:  30 train_loss:0.3162 train_acc:92.6562\n",
      "epoch: 1 batch:  31 train_loss:0.3094 train_acc:92.8931\n",
      "epoch: 1 batch:  32 train_loss:0.3035 train_acc:93.0664\n",
      "epoch: 1 batch:  33 train_loss:0.2971 train_acc:93.2292\n",
      "epoch: 1 batch:  34 train_loss:0.2909 train_acc:93.4283\n",
      "epoch: 1 batch:  35 train_loss:0.2853 train_acc:93.6161\n",
      "epoch: 1 batch:  36 train_loss:0.2797 train_acc:93.7934\n",
      "epoch: 1 batch:  37 train_loss:0.2742 train_acc:93.9611\n",
      "epoch: 1 batch:  38 train_loss:0.2688 train_acc:94.1201\n",
      "epoch: 1 batch:  39 train_loss:0.2635 train_acc:94.2708\n",
      "epoch: 1 batch:  40 train_loss:0.2586 train_acc:94.4141\n",
      "epoch: 1 batch:  41 train_loss:0.2538 train_acc:94.5503\n",
      "epoch: 1 batch:  42 train_loss:0.2490 train_acc:94.6801\n",
      "epoch: 1 batch:  43 train_loss:0.2444 train_acc:94.8038\n",
      "epoch: 1 batch:  44 train_loss:0.2402 train_acc:94.9219\n",
      "epoch: 1 batch:  45 train_loss:0.2361 train_acc:95.0347\n",
      "epoch: 1 batch:  46 train_loss:0.2321 train_acc:95.1427\n",
      "epoch: 1 batch:  47 train_loss:0.2284 train_acc:95.2460\n",
      "epoch: 1 batch:  48 train_loss:0.2254 train_acc:95.3125\n",
      "epoch: 1 batch:  49 train_loss:0.2219 train_acc:95.4082\n",
      "epoch: 1 batch:  50 train_loss:0.2184 train_acc:95.5000\n",
      "epoch: 1 batch:  51 train_loss:0.2151 train_acc:95.5882\n",
      "epoch: 1 batch:  52 train_loss:0.2118 train_acc:95.6731\n",
      "epoch: 1 batch:  53 train_loss:0.2087 train_acc:95.7547\n",
      "epoch: 1 batch:  54 train_loss:0.2057 train_acc:95.8333\n",
      "epoch: 1 batch:  55 train_loss:0.2029 train_acc:95.9091\n",
      "epoch: 1 batch:  56 train_loss:0.2000 train_acc:95.9821\n",
      "epoch: 1 batch:  57 train_loss:0.1972 train_acc:96.0526\n",
      "epoch: 1 batch:  58 train_loss:0.1946 train_acc:96.1207\n",
      "epoch: 1 batch:  59 train_loss:0.1919 train_acc:96.1864\n",
      "epoch: 1 batch:  60 train_loss:0.1893 train_acc:96.2500\n",
      "epoch: 1 batch:  61 train_loss:0.1868 train_acc:96.3115\n",
      "epoch: 1 batch:  62 train_loss:0.1843 train_acc:96.3710\n",
      "epoch: 1 batch:  63 train_loss:0.1819 train_acc:96.4286\n",
      "epoch: 1 batch:  64 train_loss:0.1797 train_acc:96.4600\n",
      "epoch: 1 batch:  65 train_loss:0.1775 train_acc:96.5144\n",
      "epoch: 1 batch:  66 train_loss:0.1752 train_acc:96.5672\n",
      "epoch: 1 batch:  67 train_loss:0.1732 train_acc:96.6185\n",
      "epoch: 1 batch:  68 train_loss:0.1711 train_acc:96.6682\n",
      "epoch: 1 batch:  69 train_loss:0.1692 train_acc:96.7165\n",
      "epoch: 1 batch:  70 train_loss:0.1672 train_acc:96.7634\n",
      "epoch: 1 batch:  71 train_loss:0.1652 train_acc:96.8090\n",
      "epoch: 1 batch:  72 train_loss:0.1633 train_acc:96.8533\n",
      "epoch: 1 batch:  73 train_loss:0.1616 train_acc:96.8964\n",
      "epoch: 1 batch:  74 train_loss:0.1598 train_acc:96.9383\n",
      "epoch: 1 batch:  75 train_loss:0.1580 train_acc:96.9792\n",
      "epoch: 1 batch:  76 train_loss:0.1562 train_acc:97.0189\n",
      "epoch: 1 batch:  77 train_loss:0.1544 train_acc:97.0576\n",
      "epoch: 1 batch:  78 train_loss:0.1528 train_acc:97.0954\n",
      "epoch: 1 batch:  79 train_loss:0.1511 train_acc:97.1321\n",
      "epoch: 1 batch:  80 train_loss:0.1496 train_acc:97.1484\n",
      "epoch: 1 batch:  81 train_loss:0.1480 train_acc:97.1836\n",
      "epoch: 1 batch:  82 train_loss:0.1465 train_acc:97.2180\n",
      "epoch: 1 batch:  83 train_loss:0.1450 train_acc:97.2515\n",
      "epoch: 1 batch:  84 train_loss:0.1435 train_acc:97.2842\n",
      "epoch: 1 batch:  85 train_loss:0.1421 train_acc:97.3162\n",
      "epoch: 1 batch:  86 train_loss:0.1407 train_acc:97.3474\n",
      "epoch: 1 batch:  87 train_loss:0.1393 train_acc:97.3779\n",
      "epoch: 1 batch:  88 train_loss:0.1380 train_acc:97.4077\n",
      "epoch: 1 batch:  89 train_loss:0.1366 train_acc:97.4368\n",
      "epoch: 1 batch:  90 train_loss:0.1353 train_acc:97.4653\n",
      "epoch: 1 batch:  91 train_loss:0.1340 train_acc:97.4931\n",
      "epoch: 1 batch:  92 train_loss:0.1328 train_acc:97.5204\n",
      "epoch: 1 batch:  93 train_loss:0.1315 train_acc:97.5470\n",
      "epoch: 1 batch:  94 train_loss:0.1303 train_acc:97.5731\n",
      "epoch: 1 batch:  95 train_loss:0.1291 train_acc:97.5987\n",
      "epoch: 1 batch:  96 train_loss:0.1279 train_acc:97.6237\n",
      "epoch: 1 batch:  97 train_loss:0.1268 train_acc:97.6482\n",
      "epoch: 1 batch:  98 train_loss:0.1257 train_acc:97.6722\n",
      "epoch: 1 batch:  99 train_loss:0.1245 train_acc:97.6957\n",
      "epoch: 1 batch: 100 train_loss:0.1235 train_acc:97.7188\n",
      "epoch: 1 batch: 101 train_loss:0.1225 train_acc:97.7413\n",
      "epoch: 1 batch: 102 train_loss:0.1214 train_acc:97.7635\n",
      "epoch: 1 batch: 103 train_loss:0.1203 train_acc:97.7852\n",
      "epoch: 1 batch: 104 train_loss:0.1193 train_acc:97.8065\n",
      "epoch: 1 batch: 105 train_loss:0.1183 train_acc:97.8274\n",
      "epoch: 1 batch: 106 train_loss:0.1173 train_acc:97.8479\n",
      "epoch: 1 batch: 107 train_loss:0.1164 train_acc:97.8680\n",
      "epoch: 1 batch: 108 train_loss:0.1154 train_acc:97.8877\n",
      "epoch: 1 batch: 109 train_loss:0.1145 train_acc:97.9071\n",
      "epoch: 1 batch: 110 train_loss:0.1136 train_acc:97.9261\n",
      "epoch: 1 batch: 111 train_loss:0.1127 train_acc:97.9448\n",
      "epoch: 1 batch: 112 train_loss:0.1118 train_acc:97.9632\n",
      "epoch: 1 batch: 113 train_loss:0.1109 train_acc:97.9812\n",
      "epoch: 1 batch: 114 train_loss:0.1100 train_acc:97.9989\n",
      "epoch: 1 batch: 115 train_loss:0.1092 train_acc:98.0163\n",
      "epoch: 1 batch: 116 train_loss:0.1083 train_acc:98.0334\n",
      "epoch: 1 batch: 117 train_loss:0.1075 train_acc:98.0502\n",
      "epoch: 1 batch: 118 train_loss:0.1067 train_acc:98.0667\n",
      "epoch: 1 batch: 119 train_loss:0.1059 train_acc:98.0830\n",
      "epoch: 1 batch: 120 train_loss:0.1051 train_acc:98.0990\n",
      "epoch: 1 batch: 121 train_loss:0.1043 train_acc:98.1147\n",
      "epoch: 1 batch: 122 train_loss:0.1035 train_acc:98.1301\n",
      "epoch: 1 batch: 123 train_loss:0.1027 train_acc:98.1453\n",
      "epoch: 1 batch: 124 train_loss:0.1020 train_acc:98.1603\n",
      "epoch: 1 batch: 125 train_loss:0.1012 train_acc:98.1750\n",
      "epoch: 1 batch: 126 train_loss:0.1005 train_acc:98.1895\n",
      "epoch: 1 batch: 127 train_loss:0.0998 train_acc:98.2037\n",
      "epoch: 1 batch: 128 train_loss:0.0991 train_acc:98.2178\n",
      "epoch: 1 batch: 129 train_loss:0.0984 train_acc:98.2316\n",
      "epoch: 1 batch: 130 train_loss:0.0977 train_acc:98.2452\n",
      "epoch: 1 batch: 131 train_loss:0.0970 train_acc:98.2586\n",
      "epoch: 1 batch: 132 train_loss:0.0963 train_acc:98.2718\n",
      "epoch: 1 batch: 133 train_loss:0.0957 train_acc:98.2848\n",
      "epoch: 1 batch: 134 train_loss:0.0950 train_acc:98.2976\n",
      "epoch: 1 batch: 135 train_loss:0.0944 train_acc:98.3102\n",
      "epoch: 1 batch: 136 train_loss:0.0937 train_acc:98.3226\n",
      "epoch: 1 batch: 137 train_loss:0.0931 train_acc:98.3349\n",
      "epoch: 1 batch: 138 train_loss:0.0925 train_acc:98.3469\n",
      "epoch: 1 batch: 139 train_loss:0.0919 train_acc:98.3588\n",
      "epoch: 1 batch: 140 train_loss:0.0913 train_acc:98.3705\n",
      "epoch: 1 batch: 141 train_loss:0.0907 train_acc:98.3821\n",
      "epoch: 1 batch: 142 train_loss:0.0901 train_acc:98.3935\n",
      "epoch: 1 batch: 143 train_loss:0.0895 train_acc:98.4047\n",
      "epoch: 1 batch: 144 train_loss:0.0889 train_acc:98.4158\n",
      "epoch: 1 batch: 145 train_loss:0.0884 train_acc:98.4267\n",
      "epoch: 1 batch: 146 train_loss:0.0882 train_acc:98.4268\n",
      "epoch: 1 batch: 147 train_loss:0.0876 train_acc:98.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 148 train_loss:0.0871 train_acc:98.4481\n",
      "epoch: 1 batch: 149 train_loss:0.0868 train_acc:98.4480\n",
      "epoch: 1 batch: 150 train_loss:0.0863 train_acc:98.4583\n",
      "epoch: 1 batch: 151 train_loss:0.0857 train_acc:98.4685\n",
      "epoch: 1 batch: 152 train_loss:0.0852 train_acc:98.4786\n",
      "epoch: 1 batch: 153 train_loss:0.0847 train_acc:98.4886\n",
      "epoch: 1 batch: 154 train_loss:0.0842 train_acc:98.4984\n",
      "epoch: 1 batch: 155 train_loss:0.0837 train_acc:98.5081\n",
      "epoch: 1 batch: 156 train_loss:0.0832 train_acc:98.5176\n",
      "epoch: 1 batch: 157 train_loss:0.0827 train_acc:98.5271\n",
      "epoch: 1 batch: 158 train_loss:0.0822 train_acc:98.5364\n",
      "epoch: 1 batch: 159 train_loss:0.0818 train_acc:98.5456\n",
      "epoch: 1 batch: 160 train_loss:0.0813 train_acc:98.5547\n",
      "epoch: 1 batch: 161 train_loss:0.0808 train_acc:98.5637\n",
      "epoch: 1 batch: 162 train_loss:0.0804 train_acc:98.5725\n",
      "epoch: 1 batch: 163 train_loss:0.0799 train_acc:98.5813\n",
      "epoch: 1 batch: 164 train_loss:0.0795 train_acc:98.5899\n",
      "epoch: 1 batch: 165 train_loss:0.0790 train_acc:98.5985\n",
      "epoch: 1 batch: 166 train_loss:0.0786 train_acc:98.6069\n",
      "epoch: 1 batch: 167 train_loss:0.0781 train_acc:98.6153\n",
      "epoch: 1 batch: 168 train_loss:0.0777 train_acc:98.6235\n",
      "epoch: 1 batch: 169 train_loss:0.0773 train_acc:98.6317\n",
      "epoch: 1 batch: 170 train_loss:0.0769 train_acc:98.6397\n",
      "epoch: 1 batch: 171 train_loss:0.0765 train_acc:98.6477\n",
      "epoch: 1 batch: 172 train_loss:0.0761 train_acc:98.6555\n",
      "epoch: 1 batch: 173 train_loss:0.0757 train_acc:98.6633\n",
      "epoch: 1 batch: 174 train_loss:0.0753 train_acc:98.6710\n",
      "epoch: 1 batch: 175 train_loss:0.0749 train_acc:98.6786\n",
      "epoch: 1 batch: 176 train_loss:0.0745 train_acc:98.6861\n",
      "epoch: 1 batch: 177 train_loss:0.0741 train_acc:98.6935\n",
      "epoch: 1 batch: 178 train_loss:0.0737 train_acc:98.7008\n",
      "epoch: 1 batch: 179 train_loss:0.0733 train_acc:98.7081\n",
      "epoch: 1 batch: 180 train_loss:0.0729 train_acc:98.7153\n",
      "epoch: 1 batch: 181 train_loss:0.0726 train_acc:98.7224\n",
      "epoch: 1 batch: 182 train_loss:0.0722 train_acc:98.7294\n",
      "epoch: 1 batch: 183 train_loss:0.0718 train_acc:98.7363\n",
      "epoch: 1 batch: 184 train_loss:0.0715 train_acc:98.7432\n",
      "epoch: 1 batch: 185 train_loss:0.0711 train_acc:98.7500\n",
      "epoch: 1 batch: 186 train_loss:0.0707 train_acc:98.7567\n",
      "epoch: 1 batch: 187 train_loss:0.0704 train_acc:98.7634\n",
      "epoch: 1 batch: 188 train_loss:0.0700 train_acc:98.7699\n",
      "epoch: 1 batch: 189 train_loss:0.0697 train_acc:98.7765\n",
      "epoch: 1 batch: 190 train_loss:0.0693 train_acc:98.7829\n",
      "epoch: 1 batch: 191 train_loss:0.0690 train_acc:98.7893\n",
      "epoch: 1 batch: 192 train_loss:0.0687 train_acc:98.7956\n",
      "epoch: 1 batch: 193 train_loss:0.0683 train_acc:98.8018\n",
      "epoch: 1 batch: 194 train_loss:0.0680 train_acc:98.8080\n",
      "epoch: 1 batch: 195 train_loss:0.0677 train_acc:98.8141\n",
      "epoch: 1 batch: 196 train_loss:0.0673 train_acc:98.8202\n",
      "epoch: 1 batch: 197 train_loss:0.0670 train_acc:98.8261\n",
      "epoch: 1 batch: 198 train_loss:0.0667 train_acc:98.8321\n",
      "epoch: 1 batch: 199 train_loss:0.0664 train_acc:98.8379\n",
      "epoch: 1 batch: 200 train_loss:0.0661 train_acc:98.8438\n",
      "epoch: 1 batch: 201 train_loss:0.0657 train_acc:98.8495\n",
      "epoch: 1 batch: 202 train_loss:0.0654 train_acc:98.8552\n",
      "epoch: 1 batch: 203 train_loss:0.0651 train_acc:98.8608\n",
      "epoch: 1 batch: 204 train_loss:0.0648 train_acc:98.8664\n",
      "epoch: 1 batch: 205 train_loss:0.0645 train_acc:98.8720\n",
      "epoch: 1 batch: 206 train_loss:0.0642 train_acc:98.8774\n",
      "epoch: 1 batch: 207 train_loss:0.0639 train_acc:98.8829\n",
      "epoch: 1 batch: 208 train_loss:0.0636 train_acc:98.8882\n",
      "epoch: 1 batch: 209 train_loss:0.0634 train_acc:98.8935\n",
      "epoch: 1 batch: 210 train_loss:0.0631 train_acc:98.8988\n",
      "epoch: 1 batch: 211 train_loss:0.0628 train_acc:98.9040\n",
      "epoch: 1 batch: 212 train_loss:0.0625 train_acc:98.9092\n",
      "epoch: 1 batch: 213 train_loss:0.0622 train_acc:98.9143\n",
      "epoch: 1 batch: 214 train_loss:0.0619 train_acc:98.9194\n",
      "epoch: 1 batch: 215 train_loss:0.0617 train_acc:98.9244\n",
      "epoch: 1 batch: 216 train_loss:0.0614 train_acc:98.9294\n",
      "epoch: 1 batch: 217 train_loss:0.0611 train_acc:98.9343\n",
      "epoch: 1 batch: 218 train_loss:0.0609 train_acc:98.9392\n",
      "epoch: 1 batch: 219 train_loss:0.0606 train_acc:98.9441\n",
      "epoch: 1 batch: 220 train_loss:0.0603 train_acc:98.9489\n",
      "epoch: 1 batch: 221 train_loss:0.0601 train_acc:98.9536\n",
      "epoch: 1 batch: 222 train_loss:0.0598 train_acc:98.9583\n",
      "epoch: 1 batch: 223 train_loss:0.0596 train_acc:98.9630\n",
      "epoch: 1 batch: 224 train_loss:0.0593 train_acc:98.9676\n",
      "epoch: 1 batch: 225 train_loss:0.0591 train_acc:98.9722\n",
      "epoch: 1 batch: 226 train_loss:0.0588 train_acc:98.9768\n",
      "epoch: 1 batch: 227 train_loss:0.0586 train_acc:98.9813\n",
      "epoch: 1 batch: 228 train_loss:0.0583 train_acc:98.9857\n",
      "epoch: 1 batch: 229 train_loss:0.0581 train_acc:98.9902\n",
      "epoch: 1 batch: 230 train_loss:0.0578 train_acc:98.9946\n",
      "epoch: 1 batch: 231 train_loss:0.0576 train_acc:98.9989\n",
      "epoch: 1 batch: 232 train_loss:0.0573 train_acc:99.0032\n",
      "epoch: 1 batch: 233 train_loss:0.0571 train_acc:99.0075\n",
      "epoch: 1 batch: 234 train_loss:0.0569 train_acc:99.0118\n",
      "epoch: 1 batch: 235 train_loss:0.0566 train_acc:99.0160\n",
      "epoch: 1 batch: 236 train_loss:0.0564 train_acc:99.0201\n",
      "epoch: 1 batch: 237 train_loss:0.0562 train_acc:99.0243\n",
      "epoch: 1 batch: 238 train_loss:0.0560 train_acc:99.0284\n",
      "epoch: 1 batch: 239 train_loss:0.0557 train_acc:99.0324\n",
      "epoch: 1 batch: 240 train_loss:0.0555 train_acc:99.0365\n",
      "epoch: 1 batch: 241 train_loss:0.0554 train_acc:99.0340\n",
      "epoch: 1 batch: 242 train_loss:0.0552 train_acc:99.0380\n",
      "epoch: 1 batch: 243 train_loss:0.0549 train_acc:99.0419\n",
      "epoch: 1 batch: 244 train_loss:0.0547 train_acc:99.0459\n",
      "epoch: 1 batch: 245 train_loss:0.0545 train_acc:99.0497\n",
      "epoch: 1 batch: 246 train_loss:0.0543 train_acc:99.0536\n",
      "epoch: 1 batch: 247 train_loss:0.0541 train_acc:99.0574\n",
      "epoch: 1 batch: 248 train_loss:0.0539 train_acc:99.0612\n",
      "epoch: 1 batch: 249 train_loss:0.0537 train_acc:99.0650\n",
      "epoch: 1 batch: 250 train_loss:0.0535 train_acc:99.0688\n",
      "epoch: 1 batch: 251 train_loss:0.0533 train_acc:99.0725\n",
      "epoch: 1 batch: 252 train_loss:0.0531 train_acc:99.0761\n",
      "epoch: 1 batch: 253 train_loss:0.0529 train_acc:99.0798\n",
      "epoch: 1 batch: 254 train_loss:0.0527 train_acc:99.0834\n",
      "epoch: 1 batch: 255 train_loss:0.0525 train_acc:99.0870\n",
      "epoch: 1 batch: 256 train_loss:0.0523 train_acc:99.0906\n",
      "epoch: 1 batch: 257 train_loss:0.0521 train_acc:99.0941\n",
      "epoch: 1 batch: 258 train_loss:0.0519 train_acc:99.0976\n",
      "epoch: 1 batch: 259 train_loss:0.0517 train_acc:99.1011\n",
      "epoch: 1 batch: 260 train_loss:0.0515 train_acc:99.1046\n",
      "epoch: 1 batch: 261 train_loss:0.0513 train_acc:99.1080\n",
      "epoch: 1 batch: 262 train_loss:0.0511 train_acc:99.1114\n",
      "epoch: 1 batch: 263 train_loss:0.0509 train_acc:99.1148\n",
      "epoch: 1 batch: 264 train_loss:0.0508 train_acc:99.1181\n",
      "epoch: 1 batch: 265 train_loss:0.0506 train_acc:99.1215\n",
      "epoch: 1 batch: 266 train_loss:0.0504 train_acc:99.1248\n",
      "epoch: 1 batch: 267 train_loss:0.0502 train_acc:99.1280\n",
      "epoch: 1 batch: 268 train_loss:0.0500 train_acc:99.1313\n",
      "epoch: 1 batch: 269 train_loss:0.0499 train_acc:99.1345\n",
      "epoch: 1 batch: 270 train_loss:0.0497 train_acc:99.1377\n",
      "epoch: 1 batch: 271 train_loss:0.0495 train_acc:99.1409\n",
      "epoch: 1 batch: 272 train_loss:0.0493 train_acc:99.1441\n",
      "epoch: 1 batch: 273 train_loss:0.0492 train_acc:99.1472\n",
      "epoch: 1 batch: 274 train_loss:0.0490 train_acc:99.1503\n",
      "epoch: 1 batch: 275 train_loss:0.0488 train_acc:99.1534\n",
      "epoch: 1 batch: 276 train_loss:0.0486 train_acc:99.1565\n",
      "epoch: 1 batch: 277 train_loss:0.0485 train_acc:99.1595\n",
      "epoch: 1 batch: 278 train_loss:0.0483 train_acc:99.1625\n",
      "epoch: 1 batch: 279 train_loss:0.0481 train_acc:99.1655\n",
      "epoch: 1 batch: 280 train_loss:0.0480 train_acc:99.1685\n",
      "epoch: 1 batch: 281 train_loss:0.0478 train_acc:99.1715\n",
      "epoch: 1 batch: 282 train_loss:0.0476 train_acc:99.1744\n",
      "epoch: 1 batch: 283 train_loss:0.0475 train_acc:99.1773\n",
      "epoch: 1 batch: 284 train_loss:0.0473 train_acc:99.1802\n",
      "epoch: 1 batch: 285 train_loss:0.0472 train_acc:99.1831\n",
      "epoch: 1 batch: 286 train_loss:0.0470 train_acc:99.1860\n",
      "epoch: 1 batch: 287 train_loss:0.0468 train_acc:99.1888\n",
      "epoch: 1 batch: 288 train_loss:0.0467 train_acc:99.1916\n",
      "epoch: 1 batch: 289 train_loss:0.0465 train_acc:99.1944\n",
      "epoch: 1 batch: 290 train_loss:0.0464 train_acc:99.1972\n",
      "epoch: 1 batch: 291 train_loss:0.0462 train_acc:99.2000\n",
      "epoch: 1 batch: 292 train_loss:0.0461 train_acc:99.2027\n",
      "epoch: 1 batch: 293 train_loss:0.0459 train_acc:99.2054\n",
      "epoch: 1 batch: 294 train_loss:0.0458 train_acc:99.2081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 295 train_loss:0.0456 train_acc:99.2108\n",
      "epoch: 1 batch: 296 train_loss:0.0455 train_acc:99.2135\n",
      "epoch: 1 batch: 297 train_loss:0.0453 train_acc:99.2161\n",
      "epoch: 1 batch: 298 train_loss:0.0452 train_acc:99.2188\n",
      "epoch: 1 batch: 299 train_loss:0.0450 train_acc:99.2214\n",
      "epoch: 1 batch: 300 train_loss:0.0449 train_acc:99.2240\n",
      "epoch: 1 batch: 301 train_loss:0.0447 train_acc:99.2265\n",
      "epoch: 1 batch: 302 train_loss:0.0446 train_acc:99.2291\n",
      "epoch: 1 batch: 303 train_loss:0.0445 train_acc:99.2316\n",
      "epoch: 1 batch: 304 train_loss:0.0443 train_acc:99.2342\n",
      "epoch: 1 batch: 305 train_loss:0.0442 train_acc:99.2367\n",
      "epoch: 1 batch: 306 train_loss:0.0440 train_acc:99.2392\n",
      "epoch: 1 batch: 307 train_loss:0.0439 train_acc:99.2417\n",
      "epoch: 1 batch: 308 train_loss:0.0438 train_acc:99.2441\n",
      "epoch: 1 batch: 309 train_loss:0.0436 train_acc:99.2466\n",
      "epoch: 1 batch: 310 train_loss:0.0435 train_acc:99.2490\n",
      "epoch: 1 batch: 311 train_loss:0.0433 train_acc:99.2514\n",
      "epoch: 1 batch: 312 train_loss:0.0432 train_acc:99.2538\n",
      "epoch: 1 batch: 313 train_loss:0.0431 train_acc:99.2562\n",
      "epoch: 1 batch: 314 train_loss:0.0429 train_acc:99.2586\n",
      "epoch: 1 batch: 315 train_loss:0.0428 train_acc:99.2609\n",
      "epoch: 1 batch: 316 train_loss:0.0427 train_acc:99.2633\n",
      "epoch: 1 batch: 317 train_loss:0.0426 train_acc:99.2656\n",
      "epoch: 1 batch: 318 train_loss:0.0424 train_acc:99.2679\n",
      "epoch: 1 batch: 319 train_loss:0.0423 train_acc:99.2702\n",
      "epoch: 1 batch: 320 train_loss:0.0422 train_acc:99.2725\n",
      "epoch: 1 batch: 321 train_loss:0.0420 train_acc:99.2747\n",
      "epoch: 1 batch: 322 train_loss:0.0422 train_acc:99.2721\n",
      "epoch: 1 batch: 323 train_loss:0.0421 train_acc:99.2744\n",
      "epoch: 1 batch: 324 train_loss:0.0419 train_acc:99.2766\n",
      "epoch: 1 batch: 325 train_loss:0.0418 train_acc:99.2788\n",
      "epoch: 1 batch: 326 train_loss:0.0417 train_acc:99.2811\n",
      "epoch: 1 batch: 327 train_loss:0.0416 train_acc:99.2833\n",
      "epoch: 1 batch: 328 train_loss:0.0414 train_acc:99.2854\n",
      "epoch: 1 batch: 329 train_loss:0.0413 train_acc:99.2876\n",
      "epoch: 1 batch: 330 train_loss:0.0412 train_acc:99.2898\n",
      "epoch: 1 batch: 331 train_loss:0.0411 train_acc:99.2919\n",
      "epoch: 1 batch: 332 train_loss:0.0411 train_acc:99.2893\n",
      "epoch: 1 batch: 333 train_loss:0.0410 train_acc:99.2915\n",
      "epoch: 1 batch: 334 train_loss:0.0409 train_acc:99.2936\n",
      "epoch: 1 batch: 335 train_loss:0.0408 train_acc:99.2957\n",
      "epoch: 1 batch: 336 train_loss:0.0407 train_acc:99.2978\n",
      "epoch: 1 batch: 337 train_loss:0.0405 train_acc:99.2999\n",
      "epoch: 1 batch: 338 train_loss:0.0404 train_acc:99.3020\n",
      "epoch: 1 batch: 339 train_loss:0.0405 train_acc:99.2994\n",
      "epoch: 1 batch: 340 train_loss:0.0403 train_acc:99.3015\n",
      "epoch: 1 batch: 341 train_loss:0.0402 train_acc:99.3035\n",
      "epoch: 1 batch: 342 train_loss:0.0401 train_acc:99.3056\n",
      "epoch: 1 batch: 343 train_loss:0.0400 train_acc:99.3076\n",
      "epoch: 1 batch: 344 train_loss:0.0399 train_acc:99.3096\n",
      "epoch: 1 batch: 345 train_loss:0.0398 train_acc:99.3116\n",
      "epoch: 1 batch: 346 train_loss:0.0397 train_acc:99.3136\n",
      "epoch: 1 batch: 347 train_loss:0.0396 train_acc:99.3156\n",
      "epoch: 1 batch: 348 train_loss:0.0395 train_acc:99.3175\n",
      "epoch: 1 batch: 349 train_loss:0.0394 train_acc:99.3195\n",
      "epoch: 1 batch: 350 train_loss:0.0393 train_acc:99.3214\n",
      "epoch: 1 batch: 351 train_loss:0.0392 train_acc:99.3234\n",
      "epoch: 1 batch: 352 train_loss:0.0391 train_acc:99.3253\n",
      "epoch: 1 batch: 353 train_loss:0.0390 train_acc:99.3272\n",
      "epoch: 1 batch: 354 train_loss:0.0389 train_acc:99.3291\n",
      "epoch: 1 batch: 355 train_loss:0.0388 train_acc:99.3310\n",
      "epoch: 1 batch: 356 train_loss:0.0387 train_acc:99.3329\n",
      "epoch: 1 batch: 357 train_loss:0.0386 train_acc:99.3347\n",
      "epoch: 1 batch: 358 train_loss:0.0385 train_acc:99.3366\n",
      "epoch: 1 batch: 359 train_loss:0.0384 train_acc:99.3384\n",
      "epoch: 1 batch: 360 train_loss:0.0383 train_acc:99.3403\n",
      "epoch: 1 batch: 361 train_loss:0.0382 train_acc:99.3421\n",
      "epoch: 1 batch: 362 train_loss:0.0381 train_acc:99.3439\n",
      "epoch: 1 batch: 363 train_loss:0.0380 train_acc:99.3457\n",
      "epoch: 1 batch: 364 train_loss:0.0379 train_acc:99.3475\n",
      "epoch: 1 batch: 365 train_loss:0.0378 train_acc:99.3493\n",
      "epoch: 1 batch: 366 train_loss:0.0377 train_acc:99.3511\n",
      "epoch: 1 batch: 367 train_loss:0.0376 train_acc:99.3529\n",
      "epoch: 1 batch: 368 train_loss:0.0375 train_acc:99.3546\n",
      "epoch: 1 batch: 369 train_loss:0.0374 train_acc:99.3564\n",
      "epoch: 1 batch: 370 train_loss:0.0374 train_acc:99.3581\n",
      "epoch: 1 batch: 371 train_loss:0.0373 train_acc:99.3598\n",
      "epoch: 1 batch: 372 train_loss:0.0372 train_acc:99.3616\n",
      "epoch: 1 batch: 373 train_loss:0.0371 train_acc:99.3633\n",
      "epoch: 1 batch: 374 train_loss:0.0370 train_acc:99.3650\n",
      "epoch: 1 batch: 375 train_loss:0.0369 train_acc:99.3667\n",
      "epoch: 1 batch: 376 train_loss:0.0368 train_acc:99.3684\n",
      "epoch: 1 batch: 377 train_loss:0.0367 train_acc:99.3700\n",
      "epoch: 1 batch: 378 train_loss:0.0366 train_acc:99.3717\n",
      "epoch: 1 batch: 379 train_loss:0.0365 train_acc:99.3734\n",
      "epoch: 1 batch: 380 train_loss:0.0364 train_acc:99.3750\n",
      "epoch: 1 batch: 381 train_loss:0.0363 train_acc:99.3766\n",
      "epoch: 1 batch: 382 train_loss:0.0362 train_acc:99.3783\n",
      "epoch: 1 batch: 383 train_loss:0.0361 train_acc:99.3799\n",
      "epoch: 1 batch: 384 train_loss:0.0361 train_acc:99.3815\n",
      "epoch: 1 batch: 385 train_loss:0.0360 train_acc:99.3831\n",
      "epoch: 1 batch: 386 train_loss:0.0359 train_acc:99.3847\n",
      "epoch: 1 batch: 387 train_loss:0.0358 train_acc:99.3863\n",
      "epoch: 1 batch: 388 train_loss:0.0357 train_acc:99.3879\n",
      "epoch: 1 batch: 389 train_loss:0.0356 train_acc:99.3895\n",
      "epoch: 1 batch: 390 train_loss:0.0355 train_acc:99.3910\n",
      "epoch: 1 batch: 391 train_loss:0.0354 train_acc:99.3926\n",
      "epoch: 1 batch: 392 train_loss:0.0353 train_acc:99.3941\n",
      "epoch: 1 batch: 393 train_loss:0.0353 train_acc:99.3957\n",
      "epoch: 1 batch: 394 train_loss:0.0352 train_acc:99.3972\n",
      "epoch: 1 batch: 395 train_loss:0.0351 train_acc:99.3987\n",
      "epoch: 1 batch: 396 train_loss:0.0350 train_acc:99.4003\n",
      "epoch: 1 batch: 397 train_loss:0.0349 train_acc:99.4018\n",
      "epoch: 1 batch: 398 train_loss:0.0348 train_acc:99.4033\n",
      "epoch: 1 batch: 399 train_loss:0.0347 train_acc:99.4048\n",
      "epoch: 1 batch: 400 train_loss:0.0347 train_acc:99.4063\n",
      "epoch: 1 batch: 401 train_loss:0.0346 train_acc:99.4077\n",
      "epoch: 1 batch: 402 train_loss:0.0345 train_acc:99.4092\n",
      "epoch: 1 batch: 403 train_loss:0.0344 train_acc:99.4107\n",
      "epoch: 1 batch: 404 train_loss:0.0343 train_acc:99.4121\n",
      "epoch: 1 batch: 405 train_loss:0.0343 train_acc:99.4136\n",
      "epoch: 1 batch: 406 train_loss:0.0342 train_acc:99.4150\n",
      "epoch: 1 batch: 407 train_loss:0.0341 train_acc:99.4165\n",
      "epoch: 1 batch: 408 train_loss:0.0340 train_acc:99.4179\n",
      "epoch: 1 batch: 409 train_loss:0.0339 train_acc:99.4193\n",
      "epoch: 1 batch: 410 train_loss:0.0339 train_acc:99.4207\n",
      "epoch: 1 batch: 411 train_loss:0.0338 train_acc:99.4221\n",
      "epoch: 1 batch: 412 train_loss:0.0337 train_acc:99.4235\n",
      "epoch: 1 batch: 413 train_loss:0.0336 train_acc:99.4249\n",
      "epoch: 1 batch: 414 train_loss:0.0335 train_acc:99.4263\n",
      "epoch: 1 batch: 415 train_loss:0.0335 train_acc:99.4277\n",
      "epoch: 1 batch: 416 train_loss:0.0334 train_acc:99.4291\n",
      "epoch: 1 batch: 417 train_loss:0.0333 train_acc:99.4305\n",
      "epoch: 1 batch: 418 train_loss:0.0332 train_acc:99.4318\n",
      "epoch: 1 batch: 419 train_loss:0.0331 train_acc:99.4332\n",
      "epoch: 1 batch: 420 train_loss:0.0331 train_acc:99.4345\n",
      "epoch: 1 batch: 421 train_loss:0.0330 train_acc:99.4359\n",
      "epoch: 1 batch: 422 train_loss:0.0329 train_acc:99.4372\n",
      "epoch: 1 batch: 423 train_loss:0.0328 train_acc:99.4385\n",
      "epoch: 1 batch: 424 train_loss:0.0328 train_acc:99.4399\n",
      "epoch: 1 batch: 425 train_loss:0.0327 train_acc:99.4412\n",
      "epoch: 1 batch: 426 train_loss:0.0326 train_acc:99.4425\n",
      "epoch: 1 batch: 427 train_loss:0.0325 train_acc:99.4438\n",
      "epoch: 1 batch: 428 train_loss:0.0325 train_acc:99.4451\n",
      "epoch: 1 batch: 429 train_loss:0.0324 train_acc:99.4464\n",
      "epoch: 1 batch: 430 train_loss:0.0323 train_acc:99.4477\n",
      "epoch: 1 batch: 431 train_loss:0.0323 train_acc:99.4490\n",
      "epoch: 1 batch: 432 train_loss:0.0322 train_acc:99.4502\n",
      "epoch: 1 batch: 433 train_loss:0.0321 train_acc:99.4515\n",
      "epoch: 1 batch: 434 train_loss:0.0320 train_acc:99.4528\n",
      "epoch: 1 batch: 435 train_loss:0.0320 train_acc:99.4540\n",
      "epoch: 1 batch: 436 train_loss:0.0319 train_acc:99.4553\n",
      "epoch: 1 batch: 437 train_loss:0.0318 train_acc:99.4565\n",
      "epoch: 1 batch: 438 train_loss:0.0318 train_acc:99.4578\n",
      "epoch: 1 batch: 439 train_loss:0.0317 train_acc:99.4590\n",
      "epoch: 1 batch: 440 train_loss:0.0316 train_acc:99.4602\n",
      "epoch: 1 batch: 441 train_loss:0.0315 train_acc:99.4615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 442 train_loss:0.0315 train_acc:99.4627\n",
      "epoch: 1 batch: 443 train_loss:0.0314 train_acc:99.4639\n",
      "epoch: 1 batch: 444 train_loss:0.0313 train_acc:99.4651\n",
      "epoch: 1 batch: 445 train_loss:0.0313 train_acc:99.4663\n",
      "epoch: 1 batch: 446 train_loss:0.0312 train_acc:99.4675\n",
      "epoch: 1 batch: 447 train_loss:0.0311 train_acc:99.4687\n",
      "epoch: 1 batch: 448 train_loss:0.0311 train_acc:99.4699\n",
      "epoch: 1 batch: 449 train_loss:0.0310 train_acc:99.4710\n",
      "epoch: 1 batch: 450 train_loss:0.0309 train_acc:99.4722\n",
      "epoch: 1 batch: 451 train_loss:0.0309 train_acc:99.4734\n",
      "epoch: 1 batch: 452 train_loss:0.0308 train_acc:99.4746\n",
      "epoch: 1 batch: 453 train_loss:0.0307 train_acc:99.4757\n",
      "epoch: 1 batch: 454 train_loss:0.0307 train_acc:99.4769\n",
      "epoch: 1 batch: 455 train_loss:0.0306 train_acc:99.4780\n",
      "epoch: 1 batch: 456 train_loss:0.0305 train_acc:99.4792\n",
      "epoch: 1 batch: 457 train_loss:0.0305 train_acc:99.4803\n",
      "epoch: 1 batch: 458 train_loss:0.0304 train_acc:99.4814\n",
      "epoch: 1 batch: 459 train_loss:0.0303 train_acc:99.4826\n",
      "epoch: 1 batch: 460 train_loss:0.0303 train_acc:99.4837\n",
      "epoch: 1 batch: 461 train_loss:0.0302 train_acc:99.4848\n",
      "epoch: 1 batch: 462 train_loss:0.0301 train_acc:99.4859\n",
      "epoch: 1 batch: 463 train_loss:0.0301 train_acc:99.4870\n",
      "epoch: 1 batch: 464 train_loss:0.0300 train_acc:99.4881\n",
      "epoch: 1 batch: 465 train_loss:0.0300 train_acc:99.4892\n",
      "epoch: 1 batch: 466 train_loss:0.0299 train_acc:99.4903\n",
      "epoch: 1 batch: 467 train_loss:0.0298 train_acc:99.4914\n",
      "epoch: 1 batch: 468 train_loss:0.0298 train_acc:99.4925\n",
      "epoch: 1 batch: 469 train_loss:0.0297 train_acc:99.4936\n",
      "epoch: 1 batch: 470 train_loss:0.0296 train_acc:99.4947\n",
      "epoch: 1 batch: 471 train_loss:0.0296 train_acc:99.4958\n",
      "epoch: 1 batch: 472 train_loss:0.0295 train_acc:99.4968\n",
      "epoch: 1 batch: 473 train_loss:0.0295 train_acc:99.4979\n",
      "epoch: 1 batch: 474 train_loss:0.0294 train_acc:99.4989\n",
      "epoch: 1 batch: 475 train_loss:0.0293 train_acc:99.5000\n",
      "epoch: 1 batch: 476 train_loss:0.0293 train_acc:99.5011\n",
      "epoch: 1 batch: 477 train_loss:0.0292 train_acc:99.5021\n",
      "epoch: 1 batch: 478 train_loss:0.0292 train_acc:99.5031\n",
      "epoch: 1 batch: 479 train_loss:0.0291 train_acc:99.5042\n",
      "epoch: 1 batch: 480 train_loss:0.0290 train_acc:99.5052\n",
      "epoch: 1 batch: 481 train_loss:0.0290 train_acc:99.5062\n",
      "epoch: 1 batch: 482 train_loss:0.0289 train_acc:99.5073\n",
      "epoch: 1 batch: 483 train_loss:0.0289 train_acc:99.5083\n",
      "epoch: 1 batch: 484 train_loss:0.0288 train_acc:99.5093\n",
      "epoch: 1 batch: 485 train_loss:0.0288 train_acc:99.5103\n",
      "epoch: 1 batch: 486 train_loss:0.0287 train_acc:99.5113\n",
      "epoch: 1 batch: 487 train_loss:0.0286 train_acc:99.5123\n",
      "epoch: 1 batch: 488 train_loss:0.0286 train_acc:99.5133\n",
      "epoch: 1 batch: 489 train_loss:0.0285 train_acc:99.5143\n",
      "epoch: 1 batch: 490 train_loss:0.0285 train_acc:99.5153\n",
      "epoch: 1 batch: 491 train_loss:0.0284 train_acc:99.5163\n",
      "epoch: 1 batch: 492 train_loss:0.0284 train_acc:99.5173\n",
      "epoch: 1 batch: 493 train_loss:0.0283 train_acc:99.5183\n",
      "epoch: 1 batch: 494 train_loss:0.0282 train_acc:99.5192\n",
      "epoch: 1 batch: 495 train_loss:0.0282 train_acc:99.5202\n",
      "epoch: 1 batch: 496 train_loss:0.0281 train_acc:99.5212\n",
      "epoch: 1 batch: 497 train_loss:0.0281 train_acc:99.5221\n",
      "epoch: 1 batch: 498 train_loss:0.0280 train_acc:99.5231\n",
      "epoch: 1 batch: 499 train_loss:0.0280 train_acc:99.5240\n",
      "epoch: 1 batch: 500 train_loss:0.0279 train_acc:99.5250\n",
      "epoch: 1 batch: 501 train_loss:0.0279 train_acc:99.5259\n",
      "epoch: 1 batch: 502 train_loss:0.0278 train_acc:99.5269\n",
      "epoch: 1 batch: 503 train_loss:0.0277 train_acc:99.5278\n",
      "epoch: 1 batch: 504 train_loss:0.0277 train_acc:99.5288\n",
      "epoch: 1 batch: 505 train_loss:0.0276 train_acc:99.5297\n",
      "epoch: 1 batch: 506 train_loss:0.0276 train_acc:99.5306\n",
      "epoch: 1 batch: 507 train_loss:0.0275 train_acc:99.5316\n",
      "epoch: 1 batch: 508 train_loss:0.0275 train_acc:99.5325\n",
      "epoch: 1 batch: 509 train_loss:0.0274 train_acc:99.5334\n",
      "epoch: 1 batch: 510 train_loss:0.0274 train_acc:99.5343\n",
      "epoch: 1 batch: 511 train_loss:0.0273 train_acc:99.5352\n",
      "epoch: 1 batch: 512 train_loss:0.0273 train_acc:99.5361\n",
      "epoch: 1 batch: 513 train_loss:0.0272 train_acc:99.5370\n",
      "epoch: 1 batch: 514 train_loss:0.0272 train_acc:99.5379\n",
      "epoch: 1 batch: 515 train_loss:0.0271 train_acc:99.5388\n",
      "epoch: 1 batch: 516 train_loss:0.0271 train_acc:99.5397\n",
      "epoch: 1 batch: 517 train_loss:0.0270 train_acc:99.5406\n",
      "epoch: 1 batch: 518 train_loss:0.0270 train_acc:99.5415\n",
      "epoch: 1 batch: 519 train_loss:0.0269 train_acc:99.5424\n",
      "epoch: 1 batch: 520 train_loss:0.0269 train_acc:99.5433\n",
      "epoch: 1 batch: 521 train_loss:0.0268 train_acc:99.5441\n",
      "epoch: 1 batch: 522 train_loss:0.0268 train_acc:99.5450\n",
      "epoch: 1 batch: 523 train_loss:0.0267 train_acc:99.5459\n",
      "epoch: 1 batch: 524 train_loss:0.0267 train_acc:99.5468\n",
      "epoch: 1 batch: 525 train_loss:0.0266 train_acc:99.5476\n",
      "epoch: 1 batch: 526 train_loss:0.0266 train_acc:99.5485\n",
      "epoch: 1 batch: 527 train_loss:0.0265 train_acc:99.5493\n",
      "epoch: 1 batch: 528 train_loss:0.0265 train_acc:99.5502\n",
      "epoch: 1 batch: 529 train_loss:0.0264 train_acc:99.5510\n",
      "epoch: 1 batch: 530 train_loss:0.0264 train_acc:99.5519\n",
      "epoch: 1 batch: 531 train_loss:0.0263 train_acc:99.5527\n",
      "epoch: 1 batch: 532 train_loss:0.0263 train_acc:99.5536\n",
      "epoch: 1 batch: 533 train_loss:0.0262 train_acc:99.5544\n",
      "epoch: 1 batch: 534 train_loss:0.0262 train_acc:99.5552\n",
      "epoch: 1 batch: 535 train_loss:0.0261 train_acc:99.5561\n",
      "epoch: 1 batch: 536 train_loss:0.0261 train_acc:99.5569\n",
      "epoch: 1 batch: 537 train_loss:0.0260 train_acc:99.5577\n",
      "epoch: 1 batch: 538 train_loss:0.0260 train_acc:99.5586\n",
      "epoch: 1 batch: 539 train_loss:0.0259 train_acc:99.5594\n",
      "epoch: 1 batch: 540 train_loss:0.0259 train_acc:99.5602\n",
      "epoch: 1 batch: 541 train_loss:0.0258 train_acc:99.5610\n",
      "epoch: 1 batch: 542 train_loss:0.0258 train_acc:99.5618\n",
      "epoch: 1 batch: 543 train_loss:0.0258 train_acc:99.5626\n",
      "epoch: 1 batch: 544 train_loss:0.0257 train_acc:99.5634\n",
      "epoch: 1 batch: 545 train_loss:0.0257 train_acc:99.5642\n",
      "epoch: 1 batch: 546 train_loss:0.0256 train_acc:99.5650\n",
      "epoch: 1 batch: 547 train_loss:0.0256 train_acc:99.5658\n",
      "epoch: 1 batch: 548 train_loss:0.0255 train_acc:99.5666\n",
      "epoch: 1 batch: 549 train_loss:0.0255 train_acc:99.5674\n",
      "epoch: 1 batch: 550 train_loss:0.0254 train_acc:99.5682\n",
      "epoch: 1 batch: 551 train_loss:0.0254 train_acc:99.5690\n",
      "epoch: 1 batch: 552 train_loss:0.0253 train_acc:99.5697\n",
      "epoch: 1 batch: 553 train_loss:0.0253 train_acc:99.5705\n",
      "epoch: 1 batch: 554 train_loss:0.0253 train_acc:99.5713\n",
      "epoch: 1 batch: 555 train_loss:0.0252 train_acc:99.5721\n",
      "epoch: 1 batch: 556 train_loss:0.0252 train_acc:99.5728\n",
      "epoch: 1 batch: 557 train_loss:0.0251 train_acc:99.5736\n",
      "epoch: 1 batch: 558 train_loss:0.0251 train_acc:99.5744\n",
      "epoch: 1 batch: 559 train_loss:0.0250 train_acc:99.5751\n",
      "epoch: 1 batch: 560 train_loss:0.0250 train_acc:99.5759\n",
      "epoch: 1 batch: 561 train_loss:0.0249 train_acc:99.5766\n",
      "epoch: 1 batch: 562 train_loss:0.0249 train_acc:99.5774\n",
      "epoch: 1 batch: 563 train_loss:0.0249 train_acc:99.5782\n",
      "epoch: 1 batch: 564 train_loss:0.0248 train_acc:99.5789\n",
      "epoch: 1 batch: 565 train_loss:0.0248 train_acc:99.5796\n",
      "epoch: 1 batch: 566 train_loss:0.0247 train_acc:99.5804\n",
      "epoch: 1 batch: 567 train_loss:0.0247 train_acc:99.5811\n",
      "epoch: 1 batch: 568 train_loss:0.0246 train_acc:99.5819\n",
      "epoch: 1 batch: 569 train_loss:0.0246 train_acc:99.5826\n",
      "epoch: 1 batch: 570 train_loss:0.0246 train_acc:99.5833\n",
      "epoch: 1 batch: 571 train_loss:0.0245 train_acc:99.5841\n",
      "epoch: 1 batch: 572 train_loss:0.0245 train_acc:99.5848\n",
      "epoch: 1 batch: 573 train_loss:0.0244 train_acc:99.5855\n",
      "epoch: 1 batch: 574 train_loss:0.0244 train_acc:99.5862\n",
      "epoch: 1 batch: 575 train_loss:0.0243 train_acc:99.5870\n",
      "epoch: 1 batch: 576 train_loss:0.0243 train_acc:99.5877\n",
      "epoch: 1 batch: 577 train_loss:0.0243 train_acc:99.5884\n",
      "epoch: 1 batch: 578 train_loss:0.0242 train_acc:99.5891\n",
      "epoch: 1 batch: 579 train_loss:0.0242 train_acc:99.5898\n",
      "epoch: 1 batch: 580 train_loss:0.0241 train_acc:99.5905\n",
      "epoch: 1 batch: 581 train_loss:0.0241 train_acc:99.5912\n",
      "epoch: 1 batch: 582 train_loss:0.0241 train_acc:99.5919\n",
      "epoch: 1 batch: 583 train_loss:0.0240 train_acc:99.5926\n",
      "epoch: 1 batch: 584 train_loss:0.0240 train_acc:99.5933\n",
      "epoch: 1 batch: 585 train_loss:0.0239 train_acc:99.5940\n",
      "epoch: 1 batch: 586 train_loss:0.0239 train_acc:99.5947\n",
      "epoch: 1 batch: 587 train_loss:0.0239 train_acc:99.5954\n",
      "epoch: 1 batch: 588 train_loss:0.0238 train_acc:99.5961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 589 train_loss:0.0238 train_acc:99.5968\n",
      "epoch: 1 batch: 590 train_loss:0.0238 train_acc:99.5975\n",
      "epoch: 1 batch: 591 train_loss:0.0237 train_acc:99.5981\n",
      "epoch: 1 batch: 592 train_loss:0.0237 train_acc:99.5988\n",
      "epoch: 1 batch: 593 train_loss:0.0236 train_acc:99.5995\n",
      "epoch: 1 batch: 594 train_loss:0.0236 train_acc:99.6002\n",
      "epoch: 1 batch: 595 train_loss:0.0236 train_acc:99.6008\n",
      "epoch: 1 batch: 596 train_loss:0.0235 train_acc:99.6015\n",
      "epoch: 1 batch: 597 train_loss:0.0235 train_acc:99.6022\n",
      "epoch: 1 batch: 598 train_loss:0.0234 train_acc:99.6028\n",
      "epoch: 1 batch: 599 train_loss:0.0234 train_acc:99.6035\n",
      "epoch: 1 batch: 600 train_loss:0.0234 train_acc:99.6042\n",
      "epoch: 1 batch: 601 train_loss:0.0233 train_acc:99.6048\n",
      "epoch: 1 batch: 602 train_loss:0.0233 train_acc:99.6055\n",
      "epoch: 1 batch: 603 train_loss:0.0232 train_acc:99.6061\n",
      "epoch: 1 batch: 604 train_loss:0.0232 train_acc:99.6068\n",
      "epoch: 1 batch: 605 train_loss:0.0232 train_acc:99.6074\n",
      "epoch: 1 batch: 606 train_loss:0.0231 train_acc:99.6081\n",
      "epoch: 1 batch: 607 train_loss:0.0231 train_acc:99.6087\n",
      "epoch: 1 batch: 608 train_loss:0.0231 train_acc:99.6094\n",
      "epoch: 1 batch: 609 train_loss:0.0230 train_acc:99.6100\n",
      "epoch: 1 batch: 610 train_loss:0.0231 train_acc:99.6081\n",
      "epoch: 1 batch: 611 train_loss:0.0230 train_acc:99.6087\n",
      "epoch: 1 batch: 612 train_loss:0.0230 train_acc:99.6094\n",
      "epoch: 1 batch: 613 train_loss:0.0230 train_acc:99.6100\n",
      "epoch: 1 batch: 614 train_loss:0.0229 train_acc:99.6106\n",
      "epoch: 1 batch: 615 train_loss:0.0229 train_acc:99.6113\n",
      "epoch: 1 batch: 616 train_loss:0.0229 train_acc:99.6094\n",
      "epoch: 1 batch: 617 train_loss:0.0229 train_acc:99.6100\n",
      "epoch: 1 batch: 618 train_loss:0.0228 train_acc:99.6106\n",
      "epoch: 1 batch: 619 train_loss:0.0228 train_acc:99.6113\n",
      "epoch: 1 batch: 620 train_loss:0.0228 train_acc:99.6119\n",
      "epoch: 1 batch: 621 train_loss:0.0227 train_acc:99.6125\n",
      "epoch: 1 batch: 622 train_loss:0.0227 train_acc:99.6131\n",
      "epoch: 1 batch: 623 train_loss:0.0227 train_acc:99.6138\n",
      "epoch: 1 batch: 624 train_loss:0.0226 train_acc:99.6144\n",
      "epoch: 1 batch: 625 train_loss:0.0226 train_acc:99.6150\n",
      "epoch: 1 batch: 626 train_loss:0.0226 train_acc:99.6156\n",
      "epoch: 1 batch: 627 train_loss:0.0225 train_acc:99.6162\n",
      "epoch: 1 batch: 628 train_loss:0.0225 train_acc:99.6168\n",
      "epoch: 1 batch: 629 train_loss:0.0225 train_acc:99.6174\n",
      "epoch: 1 batch: 630 train_loss:0.0224 train_acc:99.6181\n",
      "epoch: 1 batch: 631 train_loss:0.0224 train_acc:99.6187\n",
      "epoch: 1 batch: 632 train_loss:0.0224 train_acc:99.6193\n",
      "epoch: 1 batch: 633 train_loss:0.0223 train_acc:99.6199\n",
      "epoch: 1 batch: 634 train_loss:0.0223 train_acc:99.6205\n",
      "epoch: 1 batch: 635 train_loss:0.0222 train_acc:99.6211\n",
      "epoch: 1 batch: 636 train_loss:0.0222 train_acc:99.6217\n",
      "epoch: 1 batch: 637 train_loss:0.0222 train_acc:99.6223\n",
      "epoch: 1 batch: 638 train_loss:0.0221 train_acc:99.6228\n",
      "epoch: 1 batch: 639 train_loss:0.0221 train_acc:99.6234\n",
      "epoch: 1 batch: 640 train_loss:0.0221 train_acc:99.6240\n",
      "epoch: 1 batch: 641 train_loss:0.0220 train_acc:99.6246\n",
      "epoch: 1 batch: 642 train_loss:0.0220 train_acc:99.6252\n",
      "epoch: 1 batch: 643 train_loss:0.0220 train_acc:99.6258\n",
      "epoch: 1 batch: 644 train_loss:0.0220 train_acc:99.6239\n",
      "epoch: 1 batch: 645 train_loss:0.0220 train_acc:99.6245\n",
      "epoch: 1 batch: 646 train_loss:0.0219 train_acc:99.6251\n",
      "epoch: 1 batch: 647 train_loss:0.0219 train_acc:99.6257\n",
      "epoch: 1 batch: 648 train_loss:0.0219 train_acc:99.6263\n",
      "epoch: 1 batch: 649 train_loss:0.0218 train_acc:99.6268\n",
      "epoch: 1 batch: 650 train_loss:0.0218 train_acc:99.6274\n",
      "epoch: 1 batch: 651 train_loss:0.0218 train_acc:99.6280\n",
      "epoch: 1 batch: 652 train_loss:0.0217 train_acc:99.6285\n",
      "epoch: 1 batch: 653 train_loss:0.0217 train_acc:99.6291\n",
      "epoch: 1 batch: 654 train_loss:0.0217 train_acc:99.6297\n",
      "epoch: 1 batch: 655 train_loss:0.0216 train_acc:99.6302\n",
      "epoch: 1 batch: 656 train_loss:0.0216 train_acc:99.6308\n",
      "epoch: 1 batch: 657 train_loss:0.0216 train_acc:99.6314\n",
      "epoch: 1 batch: 658 train_loss:0.0215 train_acc:99.6319\n",
      "epoch: 1 batch: 659 train_loss:0.0215 train_acc:99.6325\n",
      "epoch: 1 batch: 660 train_loss:0.0215 train_acc:99.6330\n",
      "epoch: 1 batch: 661 train_loss:0.0214 train_acc:99.6336\n",
      "epoch: 1 batch: 662 train_loss:0.0214 train_acc:99.6342\n",
      "epoch: 1 batch: 663 train_loss:0.0214 train_acc:99.6347\n",
      "epoch: 1 batch: 664 train_loss:0.0214 train_acc:99.6353\n",
      "epoch: 1 batch: 665 train_loss:0.0213 train_acc:99.6358\n",
      "epoch: 1 batch: 666 train_loss:0.0213 train_acc:99.6364\n",
      "epoch: 1 batch: 667 train_loss:0.0213 train_acc:99.6369\n",
      "epoch: 1 batch: 668 train_loss:0.0212 train_acc:99.6374\n",
      "epoch: 1 batch: 669 train_loss:0.0212 train_acc:99.6380\n",
      "epoch: 1 batch: 670 train_loss:0.0212 train_acc:99.6385\n",
      "epoch: 1 batch: 671 train_loss:0.0211 train_acc:99.6391\n",
      "epoch: 1 batch: 672 train_loss:0.0211 train_acc:99.6396\n",
      "epoch: 1 batch: 673 train_loss:0.0211 train_acc:99.6401\n",
      "epoch: 1 batch: 674 train_loss:0.0210 train_acc:99.6407\n",
      "epoch: 1 batch: 675 train_loss:0.0210 train_acc:99.6412\n",
      "epoch: 1 batch: 676 train_loss:0.0210 train_acc:99.6417\n",
      "epoch: 1 batch: 677 train_loss:0.0210 train_acc:99.6423\n",
      "epoch: 1 batch: 678 train_loss:0.0209 train_acc:99.6428\n",
      "epoch: 1 batch: 679 train_loss:0.0209 train_acc:99.6433\n",
      "epoch: 1 batch: 680 train_loss:0.0209 train_acc:99.6438\n",
      "epoch: 1 batch: 681 train_loss:0.0208 train_acc:99.6444\n",
      "epoch: 1 batch: 682 train_loss:0.0208 train_acc:99.6449\n",
      "epoch: 1 batch: 683 train_loss:0.0208 train_acc:99.6454\n",
      "epoch: 1 batch: 684 train_loss:0.0207 train_acc:99.6459\n",
      "epoch: 1 batch: 685 train_loss:0.0207 train_acc:99.6464\n",
      "epoch: 1 batch: 686 train_loss:0.0207 train_acc:99.6470\n",
      "epoch: 1 batch: 687 train_loss:0.0207 train_acc:99.6475\n",
      "epoch: 1 batch: 688 train_loss:0.0206 train_acc:99.6480\n",
      "epoch: 1 batch: 689 train_loss:0.0206 train_acc:99.6485\n",
      "epoch: 1 batch: 690 train_loss:0.0206 train_acc:99.6490\n",
      "epoch: 1 batch: 691 train_loss:0.0205 train_acc:99.6495\n",
      "epoch: 1 batch: 692 train_loss:0.0205 train_acc:99.6500\n",
      "epoch: 1 batch: 693 train_loss:0.0205 train_acc:99.6505\n",
      "epoch: 1 batch: 694 train_loss:0.0205 train_acc:99.6510\n",
      "epoch: 1 batch: 695 train_loss:0.0204 train_acc:99.6515\n",
      "epoch: 1 batch: 696 train_loss:0.0204 train_acc:99.6520\n",
      "epoch: 1 batch: 697 train_loss:0.0204 train_acc:99.6525\n",
      "epoch: 1 batch: 698 train_loss:0.0203 train_acc:99.6530\n",
      "epoch: 1 batch: 699 train_loss:0.0203 train_acc:99.6535\n",
      "epoch: 1 batch: 700 train_loss:0.0203 train_acc:99.6540\n",
      "epoch: 1 batch: 701 train_loss:0.0203 train_acc:99.6545\n",
      "epoch: 1 batch: 702 train_loss:0.0202 train_acc:99.6550\n",
      "epoch: 1 batch: 703 train_loss:0.0202 train_acc:99.6555\n",
      "epoch: 1 batch: 704 train_loss:0.0202 train_acc:99.6560\n",
      "epoch: 1 batch: 705 train_loss:0.0201 train_acc:99.6565\n",
      "epoch: 1 batch: 706 train_loss:0.0201 train_acc:99.6570\n",
      "epoch: 1 batch: 707 train_loss:0.0201 train_acc:99.6574\n",
      "epoch: 1 batch: 708 train_loss:0.0201 train_acc:99.6579\n",
      "epoch: 1 batch: 709 train_loss:0.0200 train_acc:99.6584\n",
      "epoch: 1 batch: 710 train_loss:0.0200 train_acc:99.6589\n",
      "epoch: 1 batch: 711 train_loss:0.0200 train_acc:99.6594\n",
      "epoch: 1 batch: 712 train_loss:0.0199 train_acc:99.6598\n",
      "epoch: 1 batch: 713 train_loss:0.0199 train_acc:99.6603\n",
      "epoch: 1 batch: 714 train_loss:0.0199 train_acc:99.6608\n",
      "epoch: 1 batch: 715 train_loss:0.0199 train_acc:99.6613\n",
      "epoch: 1 batch: 716 train_loss:0.0198 train_acc:99.6617\n",
      "epoch: 1 batch: 717 train_loss:0.0198 train_acc:99.6622\n",
      "epoch: 1 batch: 718 train_loss:0.0198 train_acc:99.6627\n",
      "epoch: 1 batch: 719 train_loss:0.0198 train_acc:99.6632\n",
      "epoch: 1 batch: 720 train_loss:0.0197 train_acc:99.6636\n",
      "epoch: 1 batch: 721 train_loss:0.0197 train_acc:99.6641\n",
      "epoch: 1 batch: 722 train_loss:0.0197 train_acc:99.6646\n",
      "epoch: 1 batch: 723 train_loss:0.0196 train_acc:99.6650\n",
      "epoch: 1 batch: 724 train_loss:0.0196 train_acc:99.6655\n",
      "epoch: 1 batch: 725 train_loss:0.0196 train_acc:99.6659\n",
      "epoch: 1 batch: 726 train_loss:0.0196 train_acc:99.6664\n",
      "epoch: 1 batch: 727 train_loss:0.0195 train_acc:99.6669\n",
      "epoch: 1 batch: 728 train_loss:0.0195 train_acc:99.6673\n",
      "epoch: 1 batch: 729 train_loss:0.0195 train_acc:99.6678\n",
      "epoch: 1 batch: 730 train_loss:0.0195 train_acc:99.6682\n",
      "epoch: 1 batch: 731 train_loss:0.0194 train_acc:99.6687\n",
      "epoch: 1 batch: 732 train_loss:0.0194 train_acc:99.6691\n",
      "epoch: 1 batch: 733 train_loss:0.0194 train_acc:99.6696\n",
      "epoch: 1 batch: 734 train_loss:0.0194 train_acc:99.6700\n",
      "epoch: 1 batch: 735 train_loss:0.0193 train_acc:99.6705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 736 train_loss:0.0193 train_acc:99.6709\n",
      "epoch: 1 batch: 737 train_loss:0.0193 train_acc:99.6714\n",
      "epoch: 1 batch: 738 train_loss:0.0193 train_acc:99.6718\n",
      "epoch: 1 batch: 739 train_loss:0.0192 train_acc:99.6723\n",
      "epoch: 1 batch: 740 train_loss:0.0192 train_acc:99.6727\n",
      "epoch: 1 batch: 741 train_loss:0.0192 train_acc:99.6732\n",
      "epoch: 1 batch: 742 train_loss:0.0192 train_acc:99.6736\n",
      "epoch: 1 batch: 743 train_loss:0.0191 train_acc:99.6740\n",
      "epoch: 1 batch: 744 train_loss:0.0191 train_acc:99.6745\n",
      "epoch: 1 batch: 745 train_loss:0.0191 train_acc:99.6749\n",
      "epoch: 1 batch: 746 train_loss:0.0191 train_acc:99.6754\n",
      "epoch: 1 batch: 747 train_loss:0.0190 train_acc:99.6758\n",
      "epoch: 1 batch: 748 train_loss:0.0190 train_acc:99.6762\n",
      "epoch: 1 batch: 749 train_loss:0.0190 train_acc:99.6767\n",
      "epoch: 1 batch: 750 train_loss:0.0190 train_acc:99.6771\n",
      "epoch: 1 batch: 751 train_loss:0.0189 train_acc:99.6775\n",
      "epoch: 1 batch: 752 train_loss:0.0189 train_acc:99.6779\n",
      "epoch: 1 batch: 753 train_loss:0.0189 train_acc:99.6784\n",
      "epoch: 1 batch: 754 train_loss:0.0189 train_acc:99.6788\n",
      "epoch: 1 batch: 755 train_loss:0.0188 train_acc:99.6792\n",
      "epoch: 1 batch: 756 train_loss:0.0188 train_acc:99.6796\n",
      "epoch: 1 batch: 757 train_loss:0.0188 train_acc:99.6801\n",
      "epoch: 1 batch: 758 train_loss:0.0188 train_acc:99.6805\n",
      "epoch: 1 batch: 759 train_loss:0.0187 train_acc:99.6809\n",
      "epoch: 1 batch: 760 train_loss:0.0187 train_acc:99.6813\n",
      "epoch: 1 batch: 761 train_loss:0.0187 train_acc:99.6818\n",
      "epoch: 1 batch: 762 train_loss:0.0187 train_acc:99.6822\n",
      "epoch: 1 batch: 763 train_loss:0.0186 train_acc:99.6826\n",
      "epoch: 1 batch: 764 train_loss:0.0186 train_acc:99.6830\n",
      "epoch: 1 batch: 765 train_loss:0.0186 train_acc:99.6834\n",
      "epoch: 1 batch: 766 train_loss:0.0186 train_acc:99.6838\n",
      "epoch: 1 batch: 767 train_loss:0.0185 train_acc:99.6842\n",
      "epoch: 1 batch: 768 train_loss:0.0185 train_acc:99.6847\n",
      "epoch: 1 batch: 769 train_loss:0.0185 train_acc:99.6851\n",
      "epoch: 1 batch: 770 train_loss:0.0185 train_acc:99.6855\n",
      "epoch: 1 batch: 771 train_loss:0.0184 train_acc:99.6859\n",
      "epoch: 1 batch: 772 train_loss:0.0184 train_acc:99.6863\n",
      "epoch: 1 batch: 773 train_loss:0.0184 train_acc:99.6867\n",
      "epoch: 1 batch: 774 train_loss:0.0184 train_acc:99.6871\n",
      "epoch: 1 batch: 775 train_loss:0.0184 train_acc:99.6875\n",
      "epoch: 1 batch: 776 train_loss:0.0183 train_acc:99.6879\n",
      "epoch: 1 batch: 777 train_loss:0.0183 train_acc:99.6883\n",
      "epoch: 1 batch: 778 train_loss:0.0183 train_acc:99.6887\n",
      "epoch: 1 batch: 779 train_loss:0.0183 train_acc:99.6891\n",
      "epoch: 1 batch: 780 train_loss:0.0182 train_acc:99.6895\n",
      "epoch: 1 batch: 781 train_loss:0.0182 train_acc:99.6899\n",
      "epoch: 1 batch: 782 train_loss:0.0182 train_acc:99.6903\n",
      "epoch: 1 batch: 783 train_loss:0.0182 train_acc:99.6907\n",
      "epoch: 1 batch: 784 train_loss:0.0181 train_acc:99.6911\n",
      "epoch: 1 batch: 785 train_loss:0.0181 train_acc:99.6915\n",
      "epoch: 1 batch: 786 train_loss:0.0181 train_acc:99.6919\n",
      "epoch: 1 batch: 787 train_loss:0.0181 train_acc:99.6923\n",
      "epoch: 1 batch: 788 train_loss:0.0181 train_acc:99.6927\n",
      "epoch: 1 batch: 789 train_loss:0.0180 train_acc:99.6930\n",
      "epoch: 1 batch: 790 train_loss:0.0180 train_acc:99.6934\n",
      "epoch: 1 batch: 791 train_loss:0.0180 train_acc:99.6938\n",
      "epoch: 1 batch: 792 train_loss:0.0180 train_acc:99.6942\n",
      "epoch: 1 batch: 793 train_loss:0.0179 train_acc:99.6946\n",
      "epoch: 1 batch: 794 train_loss:0.0179 train_acc:99.6950\n",
      "epoch: 1 batch: 795 train_loss:0.0179 train_acc:99.6954\n",
      "epoch: 1 batch: 796 train_loss:0.0179 train_acc:99.6957\n",
      "epoch: 1 batch: 797 train_loss:0.0179 train_acc:99.6961\n",
      "epoch: 1 batch: 798 train_loss:0.0178 train_acc:99.6965\n",
      "epoch: 1 batch: 799 train_loss:0.0178 train_acc:99.6969\n",
      "epoch: 1 batch: 800 train_loss:0.0178 train_acc:99.6973\n",
      "epoch: 1 batch: 801 train_loss:0.0178 train_acc:99.6976\n",
      "epoch: 1 batch: 802 train_loss:0.0177 train_acc:99.6980\n",
      "epoch: 1 batch: 803 train_loss:0.0177 train_acc:99.6984\n",
      "epoch: 1 batch: 804 train_loss:0.0177 train_acc:99.6988\n",
      "epoch: 1 batch: 805 train_loss:0.0177 train_acc:99.6991\n",
      "epoch: 1 batch: 806 train_loss:0.0177 train_acc:99.6995\n",
      "epoch: 1 batch: 807 train_loss:0.0176 train_acc:99.6999\n",
      "epoch: 1 batch: 808 train_loss:0.0176 train_acc:99.7003\n",
      "epoch: 1 batch: 809 train_loss:0.0176 train_acc:99.7006\n",
      "epoch: 1 batch: 810 train_loss:0.0176 train_acc:99.7010\n",
      "epoch: 1 batch: 811 train_loss:0.0176 train_acc:99.7014\n",
      "epoch: 1 batch: 812 train_loss:0.0175 train_acc:99.7017\n",
      "epoch: 1 batch: 813 train_loss:0.0175 train_acc:99.7021\n",
      "epoch: 1 batch: 814 train_loss:0.0175 train_acc:99.7025\n",
      "epoch: 1 batch: 815 train_loss:0.0175 train_acc:99.7028\n",
      "epoch: 1 batch: 816 train_loss:0.0174 train_acc:99.7032\n",
      "epoch: 1 batch: 817 train_loss:0.0174 train_acc:99.7036\n",
      "epoch: 1 batch: 818 train_loss:0.0174 train_acc:99.7039\n",
      "epoch: 1 batch: 819 train_loss:0.0174 train_acc:99.7043\n",
      "epoch: 1 batch: 820 train_loss:0.0174 train_acc:99.7046\n",
      "epoch: 1 batch: 821 train_loss:0.0173 train_acc:99.7050\n",
      "epoch: 1 batch: 822 train_loss:0.0173 train_acc:99.7054\n",
      "epoch: 1 batch: 823 train_loss:0.0173 train_acc:99.7057\n",
      "epoch: 1 batch: 824 train_loss:0.0173 train_acc:99.7061\n",
      "epoch: 1 batch: 825 train_loss:0.0173 train_acc:99.7064\n",
      "epoch: 1 batch: 826 train_loss:0.0172 train_acc:99.7068\n",
      "epoch: 1 batch: 827 train_loss:0.0172 train_acc:99.7071\n",
      "epoch: 1 batch: 828 train_loss:0.0172 train_acc:99.7075\n",
      "epoch: 1 batch: 829 train_loss:0.0172 train_acc:99.7079\n",
      "epoch: 1 batch: 830 train_loss:0.0172 train_acc:99.7082\n",
      "epoch: 1 batch: 831 train_loss:0.0171 train_acc:99.7086\n",
      "epoch: 1 batch: 832 train_loss:0.0171 train_acc:99.7089\n",
      "epoch: 1 batch: 833 train_loss:0.0171 train_acc:99.7093\n",
      "epoch: 1 batch: 834 train_loss:0.0171 train_acc:99.7096\n",
      "epoch: 1 batch: 835 train_loss:0.0171 train_acc:99.7100\n",
      "epoch: 1 batch: 836 train_loss:0.0170 train_acc:99.7103\n",
      "epoch: 1 batch: 837 train_loss:0.0170 train_acc:99.7106\n",
      "epoch: 1 batch: 838 train_loss:0.0170 train_acc:99.7110\n",
      "epoch: 1 batch: 839 train_loss:0.0170 train_acc:99.7113\n",
      "epoch: 1 batch: 840 train_loss:0.0170 train_acc:99.7117\n",
      "epoch: 1 batch: 841 train_loss:0.0169 train_acc:99.7120\n",
      "epoch: 1 batch: 842 train_loss:0.0169 train_acc:99.7124\n",
      "epoch: 1 batch: 843 train_loss:0.0169 train_acc:99.7127\n",
      "epoch: 1 batch: 844 train_loss:0.0169 train_acc:99.7130\n",
      "epoch: 1 batch: 845 train_loss:0.0169 train_acc:99.7134\n",
      "epoch: 1 batch: 846 train_loss:0.0168 train_acc:99.7137\n",
      "epoch: 1 batch: 847 train_loss:0.0168 train_acc:99.7141\n",
      "epoch: 1 batch: 848 train_loss:0.0168 train_acc:99.7144\n",
      "epoch: 1 batch: 849 train_loss:0.0168 train_acc:99.7147\n",
      "epoch: 1 batch: 850 train_loss:0.0168 train_acc:99.7151\n",
      "epoch: 1 batch: 851 train_loss:0.0167 train_acc:99.7154\n",
      "epoch: 1 batch: 852 train_loss:0.0167 train_acc:99.7157\n",
      "epoch: 1 batch: 853 train_loss:0.0167 train_acc:99.7161\n",
      "epoch: 1 batch: 854 train_loss:0.0167 train_acc:99.7164\n",
      "epoch: 1 batch: 855 train_loss:0.0167 train_acc:99.7167\n",
      "epoch: 1 batch: 856 train_loss:0.0166 train_acc:99.7171\n",
      "epoch: 1 batch: 857 train_loss:0.0166 train_acc:99.7174\n",
      "epoch: 1 batch: 858 train_loss:0.0166 train_acc:99.7177\n",
      "epoch: 1 batch: 859 train_loss:0.0166 train_acc:99.7181\n",
      "epoch: 1 batch: 860 train_loss:0.0166 train_acc:99.7184\n",
      "epoch: 1 batch: 861 train_loss:0.0165 train_acc:99.7187\n",
      "epoch: 1 batch: 862 train_loss:0.0165 train_acc:99.7190\n",
      "epoch: 1 batch: 863 train_loss:0.0165 train_acc:99.7194\n",
      "epoch: 1 batch: 864 train_loss:0.0165 train_acc:99.7197\n",
      "epoch: 1 batch: 865 train_loss:0.0165 train_acc:99.7200\n",
      "epoch: 1 batch: 866 train_loss:0.0165 train_acc:99.7203\n",
      "epoch: 1 batch: 867 train_loss:0.0164 train_acc:99.7207\n",
      "epoch: 1 batch: 868 train_loss:0.0164 train_acc:99.7210\n",
      "epoch: 1 batch: 869 train_loss:0.0164 train_acc:99.7213\n",
      "epoch: 1 batch: 870 train_loss:0.0164 train_acc:99.7216\n",
      "epoch: 1 batch: 871 train_loss:0.0164 train_acc:99.7219\n",
      "epoch: 1 batch: 872 train_loss:0.0163 train_acc:99.7223\n",
      "epoch: 1 batch: 873 train_loss:0.0163 train_acc:99.7226\n",
      "epoch: 1 batch: 874 train_loss:0.0163 train_acc:99.7229\n",
      "epoch: 1 batch: 875 train_loss:0.0163 train_acc:99.7232\n",
      "epoch: 1 batch: 876 train_loss:0.0163 train_acc:99.7235\n",
      "epoch: 1 batch: 877 train_loss:0.0163 train_acc:99.7238\n",
      "epoch: 1 batch: 878 train_loss:0.0162 train_acc:99.7242\n",
      "epoch: 1 batch: 879 train_loss:0.0162 train_acc:99.7245\n",
      "epoch: 1 batch: 880 train_loss:0.0162 train_acc:99.7248\n",
      "epoch: 1 batch: 881 train_loss:0.0162 train_acc:99.7251\n",
      "epoch: 1 batch: 882 train_loss:0.0162 train_acc:99.7254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 883 train_loss:0.0161 train_acc:99.7257\n",
      "epoch: 1 batch: 884 train_loss:0.0161 train_acc:99.7260\n",
      "epoch: 1 batch: 885 train_loss:0.0161 train_acc:99.7263\n",
      "epoch: 1 batch: 886 train_loss:0.0161 train_acc:99.7267\n",
      "epoch: 1 batch: 887 train_loss:0.0161 train_acc:99.7270\n",
      "epoch: 1 batch: 888 train_loss:0.0161 train_acc:99.7273\n",
      "epoch: 1 batch: 889 train_loss:0.0160 train_acc:99.7276\n",
      "epoch: 1 batch: 890 train_loss:0.0160 train_acc:99.7279\n",
      "epoch: 1 batch: 891 train_loss:0.0160 train_acc:99.7282\n",
      "epoch: 1 batch: 892 train_loss:0.0160 train_acc:99.7285\n",
      "epoch: 1 batch: 893 train_loss:0.0160 train_acc:99.7288\n",
      "epoch: 1 batch: 894 train_loss:0.0159 train_acc:99.7291\n",
      "epoch: 1 batch: 895 train_loss:0.0159 train_acc:99.7294\n",
      "epoch: 1 batch: 896 train_loss:0.0159 train_acc:99.7297\n",
      "epoch: 1 batch: 897 train_loss:0.0159 train_acc:99.7300\n",
      "epoch: 1 batch: 898 train_loss:0.0159 train_acc:99.7303\n",
      "epoch: 1 batch: 899 train_loss:0.0159 train_acc:99.7306\n",
      "epoch: 1 batch: 900 train_loss:0.0158 train_acc:99.7309\n",
      "epoch: 1 batch: 901 train_loss:0.0158 train_acc:99.7312\n",
      "epoch: 1 batch: 902 train_loss:0.0158 train_acc:99.7315\n",
      "epoch: 1 batch: 903 train_loss:0.0158 train_acc:99.7318\n",
      "epoch: 1 batch: 904 train_loss:0.0158 train_acc:99.7321\n",
      "epoch: 1 batch: 905 train_loss:0.0158 train_acc:99.7324\n",
      "epoch: 1 batch: 906 train_loss:0.0157 train_acc:99.7327\n",
      "epoch: 1 batch: 907 train_loss:0.0157 train_acc:99.7330\n",
      "epoch: 1 batch: 908 train_loss:0.0157 train_acc:99.7333\n",
      "epoch: 1 batch: 909 train_loss:0.0157 train_acc:99.7336\n",
      "epoch: 1 batch: 910 train_loss:0.0157 train_acc:99.7339\n",
      "epoch: 1 batch: 911 train_loss:0.0157 train_acc:99.7342\n",
      "epoch: 1 batch: 912 train_loss:0.0156 train_acc:99.7344\n",
      "epoch: 1 batch: 913 train_loss:0.0156 train_acc:99.7347\n",
      "epoch: 1 batch: 914 train_loss:0.0156 train_acc:99.7350\n",
      "epoch: 1 batch: 915 train_loss:0.0156 train_acc:99.7353\n",
      "epoch: 1 batch: 916 train_loss:0.0156 train_acc:99.7356\n",
      "epoch: 1 batch: 917 train_loss:0.0156 train_acc:99.7359\n",
      "epoch: 1 batch: 918 train_loss:0.0155 train_acc:99.7362\n",
      "epoch: 1 batch: 919 train_loss:0.0155 train_acc:99.7365\n",
      "epoch: 1 batch: 920 train_loss:0.0155 train_acc:99.7368\n",
      "epoch: 1 batch: 921 train_loss:0.0155 train_acc:99.7370\n",
      "epoch: 1 batch: 922 train_loss:0.0155 train_acc:99.7373\n",
      "epoch: 1 batch: 923 train_loss:0.0155 train_acc:99.7376\n",
      "epoch: 1 batch: 924 train_loss:0.0154 train_acc:99.7379\n",
      "epoch: 1 batch: 925 train_loss:0.0154 train_acc:99.7382\n",
      "epoch: 1 batch: 926 train_loss:0.0154 train_acc:99.7385\n",
      "epoch: 1 batch: 927 train_loss:0.0154 train_acc:99.7387\n",
      "epoch: 1 batch: 928 train_loss:0.0154 train_acc:99.7390\n",
      "epoch: 1 batch: 929 train_loss:0.0154 train_acc:99.7393\n",
      "epoch: 1 batch: 930 train_loss:0.0153 train_acc:99.7396\n",
      "epoch: 1 batch: 931 train_loss:0.0153 train_acc:99.7399\n",
      "epoch: 1 batch: 932 train_loss:0.0153 train_acc:99.7401\n",
      "epoch: 1 batch: 933 train_loss:0.0153 train_acc:99.7404\n",
      "epoch: 1 batch: 934 train_loss:0.0153 train_acc:99.7407\n",
      "epoch: 1 batch: 935 train_loss:0.0153 train_acc:99.7410\n",
      "epoch: 1 batch: 936 train_loss:0.0152 train_acc:99.7413\n",
      "epoch: 1 batch: 937 train_loss:0.0152 train_acc:99.7415\n",
      "epoch: 1 batch: 938 train_loss:0.0152 train_acc:99.7418\n",
      "epoch: 1 batch: 939 train_loss:0.0152 train_acc:99.7421\n",
      "epoch: 1 batch: 940 train_loss:0.0152 train_acc:99.7424\n",
      "epoch: 1 batch: 941 train_loss:0.0152 train_acc:99.7426\n",
      "epoch: 1 batch: 942 train_loss:0.0151 train_acc:99.7429\n",
      "epoch: 1 batch: 943 train_loss:0.0151 train_acc:99.7432\n",
      "epoch: 1 batch: 944 train_loss:0.0151 train_acc:99.7434\n",
      "epoch: 1 batch: 945 train_loss:0.0151 train_acc:99.7437\n",
      "epoch: 1 batch: 946 train_loss:0.0151 train_acc:99.7440\n",
      "epoch: 1 batch: 947 train_loss:0.0151 train_acc:99.7443\n",
      "epoch: 1 batch: 948 train_loss:0.0151 train_acc:99.7445\n",
      "epoch: 1 batch: 949 train_loss:0.0150 train_acc:99.7448\n",
      "epoch: 1 batch: 950 train_loss:0.0150 train_acc:99.7451\n",
      "epoch: 1 batch: 951 train_loss:0.0150 train_acc:99.7453\n",
      "epoch: 1 batch: 952 train_loss:0.0150 train_acc:99.7456\n",
      "epoch: 1 batch: 953 train_loss:0.0150 train_acc:99.7459\n",
      "epoch: 1 batch: 954 train_loss:0.0150 train_acc:99.7461\n",
      "epoch: 1 batch: 955 train_loss:0.0149 train_acc:99.7464\n",
      "epoch: 1 batch: 956 train_loss:0.0149 train_acc:99.7467\n",
      "epoch: 1 batch: 957 train_loss:0.0149 train_acc:99.7469\n",
      "epoch: 1 batch: 958 train_loss:0.0149 train_acc:99.7472\n",
      "epoch: 1 batch: 959 train_loss:0.0149 train_acc:99.7475\n",
      "epoch: 1 batch: 960 train_loss:0.0149 train_acc:99.7477\n",
      "epoch: 1 batch: 961 train_loss:0.0148 train_acc:99.7480\n",
      "epoch: 1 batch: 962 train_loss:0.0148 train_acc:99.7482\n",
      "epoch: 1 batch: 963 train_loss:0.0148 train_acc:99.7485\n",
      "epoch: 1 batch: 964 train_loss:0.0148 train_acc:99.7488\n",
      "epoch: 1 batch: 965 train_loss:0.0148 train_acc:99.7490\n",
      "epoch: 1 batch: 966 train_loss:0.0148 train_acc:99.7493\n",
      "epoch: 1 batch: 967 train_loss:0.0148 train_acc:99.7495\n",
      "epoch: 1 batch: 968 train_loss:0.0147 train_acc:99.7498\n",
      "epoch: 1 batch: 969 train_loss:0.0147 train_acc:99.7501\n",
      "epoch: 1 batch: 970 train_loss:0.0147 train_acc:99.7503\n",
      "epoch: 1 batch: 971 train_loss:0.0147 train_acc:99.7506\n",
      "epoch: 1 batch: 972 train_loss:0.0147 train_acc:99.7508\n",
      "epoch: 1 batch: 973 train_loss:0.0147 train_acc:99.7511\n",
      "epoch: 1 batch: 974 train_loss:0.0147 train_acc:99.7513\n",
      "epoch: 1 batch: 975 train_loss:0.0146 train_acc:99.7516\n",
      "epoch: 1 batch: 976 train_loss:0.0146 train_acc:99.7519\n",
      "epoch: 1 batch: 977 train_loss:0.0146 train_acc:99.7521\n",
      "epoch: 1 batch: 978 train_loss:0.0146 train_acc:99.7524\n",
      "epoch: 1 batch: 979 train_loss:0.0146 train_acc:99.7526\n",
      "epoch: 1 batch: 980 train_loss:0.0146 train_acc:99.7529\n",
      "epoch: 1 batch: 981 train_loss:0.0146 train_acc:99.7531\n",
      "epoch: 1 batch: 982 train_loss:0.0145 train_acc:99.7534\n",
      "epoch: 1 batch: 983 train_loss:0.0145 train_acc:99.7536\n",
      "epoch: 1 batch: 984 train_loss:0.0145 train_acc:99.7539\n",
      "epoch: 1 batch: 985 train_loss:0.0145 train_acc:99.7541\n",
      "epoch: 1 batch: 986 train_loss:0.0145 train_acc:99.7544\n",
      "epoch: 1 batch: 987 train_loss:0.0145 train_acc:99.7546\n",
      "epoch: 1 batch: 988 train_loss:0.0144 train_acc:99.7549\n",
      "epoch: 1 batch: 989 train_loss:0.0144 train_acc:99.7551\n",
      "epoch: 1 batch: 990 train_loss:0.0144 train_acc:99.7554\n",
      "epoch: 1 batch: 991 train_loss:0.0144 train_acc:99.7556\n",
      "epoch: 1 batch: 992 train_loss:0.0144 train_acc:99.7559\n",
      "epoch: 1 batch: 993 train_loss:0.0144 train_acc:99.7561\n",
      "epoch: 1 batch: 994 train_loss:0.0144 train_acc:99.7564\n",
      "epoch: 1 batch: 995 train_loss:0.0143 train_acc:99.7566\n",
      "epoch: 1 batch: 996 train_loss:0.0143 train_acc:99.7568\n",
      "epoch: 1 batch: 997 train_loss:0.0143 train_acc:99.7571\n",
      "epoch: 1 batch: 998 train_loss:0.0143 train_acc:99.7573\n",
      "epoch: 1 batch: 999 train_loss:0.0143 train_acc:99.7576\n",
      "epoch: 1 batch:1000 train_loss:0.0143 train_acc:99.7578\n",
      "epoch: 1 batch:1001 train_loss:0.0143 train_acc:99.7581\n",
      "epoch: 1 batch:1002 train_loss:0.0143 train_acc:99.7583\n",
      "epoch: 1 batch:1003 train_loss:0.0142 train_acc:99.7585\n",
      "epoch: 1 batch:1004 train_loss:0.0142 train_acc:99.7588\n",
      "epoch: 1 batch:1005 train_loss:0.0142 train_acc:99.7590\n",
      "epoch: 1 batch:1006 train_loss:0.0142 train_acc:99.7593\n",
      "epoch: 1 batch:1007 train_loss:0.0142 train_acc:99.7595\n",
      "epoch: 1 batch:1008 train_loss:0.0142 train_acc:99.7597\n",
      "epoch: 1 batch:1009 train_loss:0.0142 train_acc:99.7600\n",
      "epoch: 1 batch:1010 train_loss:0.0141 train_acc:99.7602\n",
      "epoch: 1 batch:1011 train_loss:0.0141 train_acc:99.7604\n",
      "epoch: 1 batch:1012 train_loss:0.0141 train_acc:99.7607\n",
      "epoch: 1 batch:1013 train_loss:0.0141 train_acc:99.7609\n",
      "epoch: 1 batch:1014 train_loss:0.0141 train_acc:99.7612\n",
      "epoch: 1 batch:1015 train_loss:0.0141 train_acc:99.7614\n",
      "epoch: 1 batch:1016 train_loss:0.0141 train_acc:99.7616\n",
      "epoch: 1 batch:1017 train_loss:0.0140 train_acc:99.7619\n",
      "epoch: 1 batch:1018 train_loss:0.0140 train_acc:99.7621\n",
      "epoch: 1 batch:1019 train_loss:0.0140 train_acc:99.7623\n",
      "epoch: 1 batch:1020 train_loss:0.0140 train_acc:99.7626\n",
      "epoch: 1 batch:1021 train_loss:0.0140 train_acc:99.7628\n",
      "epoch: 1 batch:1022 train_loss:0.0140 train_acc:99.7630\n",
      "epoch: 1 batch:1023 train_loss:0.0140 train_acc:99.7633\n",
      "epoch: 1 batch:1024 train_loss:0.0139 train_acc:99.7635\n",
      "epoch: 1 batch:1025 train_loss:0.0139 train_acc:99.7637\n",
      "epoch: 1 batch:1026 train_loss:0.0139 train_acc:99.7639\n",
      "epoch: 1 batch:1027 train_loss:0.0139 train_acc:99.7642\n",
      "epoch: 1 batch:1028 train_loss:0.0139 train_acc:99.7644\n",
      "epoch: 1 batch:1029 train_loss:0.0139 train_acc:99.7646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch:1030 train_loss:0.0139 train_acc:99.7649\n",
      "epoch: 1 batch:1031 train_loss:0.0139 train_acc:99.7651\n",
      "epoch: 1 batch:1032 train_loss:0.0138 train_acc:99.7653\n",
      "epoch: 1 batch:1033 train_loss:0.0138 train_acc:99.7655\n",
      "epoch: 1 batch:1034 train_loss:0.0138 train_acc:99.7658\n",
      "epoch: 1 batch:1035 train_loss:0.0138 train_acc:99.7660\n",
      "epoch: 1 batch:1036 train_loss:0.0138 train_acc:99.7662\n",
      "epoch: 1 batch:1037 train_loss:0.0138 train_acc:99.7665\n",
      "epoch: 1 batch:1038 train_loss:0.0138 train_acc:99.7667\n",
      "epoch: 1 batch:1039 train_loss:0.0138 train_acc:99.7669\n",
      "epoch: 1 batch:1040 train_loss:0.0137 train_acc:99.7671\n",
      "epoch: 1 batch:1041 train_loss:0.0137 train_acc:99.7674\n",
      "epoch: 1 batch:1042 train_loss:0.0137 train_acc:99.7676\n",
      "epoch: 1 batch:1043 train_loss:0.0137 train_acc:99.7678\n",
      "epoch: 1 batch:1044 train_loss:0.0137 train_acc:99.7680\n",
      "epoch: 1 batch:1045 train_loss:0.0137 train_acc:99.7682\n",
      "epoch: 1 batch:1046 train_loss:0.0137 train_acc:99.7685\n",
      "epoch: 1 batch:1047 train_loss:0.0136 train_acc:99.7687\n",
      "epoch: 1 batch:1048 train_loss:0.0136 train_acc:99.7689\n",
      "epoch: 1 batch:1049 train_loss:0.0136 train_acc:99.7691\n",
      "epoch: 1 batch:1050 train_loss:0.0136 train_acc:99.7693\n",
      "epoch: 1 batch:1051 train_loss:0.0136 train_acc:99.7696\n",
      "epoch: 1 batch:1052 train_loss:0.0136 train_acc:99.7698\n",
      "epoch: 1 batch:1053 train_loss:0.0136 train_acc:99.7700\n",
      "epoch: 1 batch:1054 train_loss:0.0136 train_acc:99.7702\n",
      "epoch: 1 batch:1055 train_loss:0.0135 train_acc:99.7704\n",
      "epoch: 1 batch:1056 train_loss:0.0135 train_acc:99.7707\n",
      "epoch: 1 batch:1057 train_loss:0.0135 train_acc:99.7709\n",
      "epoch: 1 batch:1058 train_loss:0.0135 train_acc:99.7711\n",
      "epoch: 1 batch:1059 train_loss:0.0135 train_acc:99.7713\n",
      "epoch: 1 batch:1060 train_loss:0.0135 train_acc:99.7715\n",
      "epoch: 1 batch:1061 train_loss:0.0135 train_acc:99.7717\n",
      "epoch: 1 batch:1062 train_loss:0.0135 train_acc:99.7720\n",
      "epoch: 1 batch:1063 train_loss:0.0134 train_acc:99.7722\n",
      "epoch: 1 batch:1064 train_loss:0.0134 train_acc:99.7724\n",
      "epoch: 1 batch:1065 train_loss:0.0134 train_acc:99.7726\n",
      "epoch: 1 batch:1066 train_loss:0.0134 train_acc:99.7728\n",
      "epoch: 1 batch:1067 train_loss:0.0134 train_acc:99.7730\n",
      "epoch: 1 batch:1068 train_loss:0.0134 train_acc:99.7732\n",
      "epoch: 1 batch:1069 train_loss:0.0134 train_acc:99.7734\n",
      "epoch: 1 batch:1070 train_loss:0.0134 train_acc:99.7737\n",
      "epoch: 1 batch:1071 train_loss:0.0133 train_acc:99.7739\n",
      "epoch: 1 batch:1072 train_loss:0.0133 train_acc:99.7741\n",
      "epoch: 1 batch:1073 train_loss:0.0133 train_acc:99.7743\n",
      "epoch: 1 batch:1074 train_loss:0.0133 train_acc:99.7745\n",
      "epoch: 1 batch:1075 train_loss:0.0133 train_acc:99.7747\n",
      "epoch: 1 batch:1076 train_loss:0.0133 train_acc:99.7749\n",
      "epoch: 1 batch:1077 train_loss:0.0133 train_acc:99.7751\n",
      "epoch: 1 batch:1078 train_loss:0.0133 train_acc:99.7753\n",
      "epoch: 1 batch:1079 train_loss:0.0132 train_acc:99.7755\n",
      "epoch: 1 batch:1080 train_loss:0.0132 train_acc:99.7758\n",
      "epoch: 1 batch:1081 train_loss:0.0132 train_acc:99.7760\n",
      "epoch: 1 batch:1082 train_loss:0.0132 train_acc:99.7762\n",
      "epoch: 1 batch:1083 train_loss:0.0132 train_acc:99.7764\n",
      "epoch: 1 batch:1084 train_loss:0.0132 train_acc:99.7766\n",
      "epoch: 1 batch:1085 train_loss:0.0132 train_acc:99.7768\n",
      "epoch: 1 batch:1086 train_loss:0.0132 train_acc:99.7770\n",
      "epoch: 1 batch:1087 train_loss:0.0132 train_acc:99.7772\n",
      "epoch: 1 batch:1088 train_loss:0.0131 train_acc:99.7774\n",
      "epoch: 1 batch:1089 train_loss:0.0131 train_acc:99.7776\n",
      "epoch: 1 batch:1090 train_loss:0.0131 train_acc:99.7778\n",
      "epoch: 1 batch:1091 train_loss:0.0131 train_acc:99.7780\n",
      "epoch: 1 batch:1092 train_loss:0.0131 train_acc:99.7782\n",
      "epoch: 1 batch:1093 train_loss:0.0131 train_acc:99.7784\n",
      "epoch: 1 batch:1094 train_loss:0.0131 train_acc:99.7786\n",
      "epoch: 1 batch:1095 train_loss:0.0131 train_acc:99.7788\n",
      "epoch: 1 batch:1096 train_loss:0.0130 train_acc:99.7790\n",
      "epoch: 1 batch:1097 train_loss:0.0130 train_acc:99.7792\n",
      "epoch: 1 batch:1098 train_loss:0.0130 train_acc:99.7794\n",
      "epoch: 1 batch:1099 train_loss:0.0130 train_acc:99.7796\n",
      "epoch: 1 batch:1100 train_loss:0.0130 train_acc:99.7798\n",
      "epoch: 1 batch:1101 train_loss:0.0130 train_acc:99.7800\n",
      "epoch: 1 batch:1102 train_loss:0.0130 train_acc:99.7802\n",
      "epoch: 1 batch:1103 train_loss:0.0130 train_acc:99.7804\n",
      "epoch: 1 batch:1104 train_loss:0.0130 train_acc:99.7806\n",
      "epoch: 1 batch:1105 train_loss:0.0129 train_acc:99.7808\n",
      "epoch: 1 batch:1106 train_loss:0.0129 train_acc:99.7810\n",
      "epoch: 1 batch:1107 train_loss:0.0129 train_acc:99.7812\n",
      "epoch: 1 batch:1108 train_loss:0.0129 train_acc:99.7814\n",
      "epoch: 1 batch:1109 train_loss:0.0129 train_acc:99.7816\n",
      "epoch: 1 batch:1110 train_loss:0.0129 train_acc:99.7818\n",
      "epoch: 1 batch:1111 train_loss:0.0129 train_acc:99.7820\n",
      "epoch: 1 batch:1112 train_loss:0.0129 train_acc:99.7822\n",
      "epoch: 1 batch:1113 train_loss:0.0128 train_acc:99.7824\n",
      "epoch: 1 batch:1114 train_loss:0.0128 train_acc:99.7826\n",
      "epoch: 1 batch:1115 train_loss:0.0128 train_acc:99.7828\n",
      "epoch: 1 batch:1116 train_loss:0.0128 train_acc:99.7830\n",
      "epoch: 1 batch:1117 train_loss:0.0128 train_acc:99.7832\n",
      "epoch: 1 batch:1118 train_loss:0.0128 train_acc:99.7834\n",
      "epoch: 1 batch:1119 train_loss:0.0128 train_acc:99.7836\n",
      "epoch: 1 batch:1120 train_loss:0.0128 train_acc:99.7838\n",
      "epoch: 1 batch:1121 train_loss:0.0128 train_acc:99.7840\n",
      "epoch: 1 batch:1122 train_loss:0.0127 train_acc:99.7841\n",
      "epoch: 1 batch:1123 train_loss:0.0127 train_acc:99.7843\n",
      "epoch: 1 batch:1124 train_loss:0.0127 train_acc:99.7845\n",
      "epoch: 1 batch:1125 train_loss:0.0127 train_acc:99.7847\n",
      "epoch: 1 batch:1126 train_loss:0.0127 train_acc:99.7849\n",
      "epoch: 1 batch:1127 train_loss:0.0127 train_acc:99.7851\n",
      "epoch: 1 batch:1128 train_loss:0.0127 train_acc:99.7853\n",
      "epoch: 1 batch:1129 train_loss:0.0127 train_acc:99.7855\n",
      "epoch: 1 batch:1130 train_loss:0.0127 train_acc:99.7857\n",
      "epoch: 1 batch:1131 train_loss:0.0126 train_acc:99.7859\n",
      "epoch: 1 batch:1132 train_loss:0.0126 train_acc:99.7861\n",
      "epoch: 1 batch:1133 train_loss:0.0126 train_acc:99.7862\n",
      "epoch: 1 batch:1134 train_loss:0.0126 train_acc:99.7864\n",
      "epoch: 1 batch:1135 train_loss:0.0126 train_acc:99.7866\n",
      "epoch: 1 batch:1136 train_loss:0.0126 train_acc:99.7868\n",
      "epoch: 1 batch:1137 train_loss:0.0126 train_acc:99.7870\n",
      "epoch: 1 batch:1138 train_loss:0.0126 train_acc:99.7872\n",
      "epoch: 1 batch:1139 train_loss:0.0126 train_acc:99.7874\n",
      "epoch: 1 batch:1140 train_loss:0.0125 train_acc:99.7876\n",
      "epoch: 1 batch:1141 train_loss:0.0125 train_acc:99.7877\n",
      "epoch: 1 batch:1142 train_loss:0.0125 train_acc:99.7879\n",
      "epoch: 1 batch:1143 train_loss:0.0125 train_acc:99.7881\n",
      "epoch: 1 batch:1144 train_loss:0.0125 train_acc:99.7883\n",
      "epoch: 1 batch:1145 train_loss:0.0125 train_acc:99.7885\n",
      "epoch: 1 batch:1146 train_loss:0.0125 train_acc:99.7887\n",
      "epoch: 1 batch:1147 train_loss:0.0125 train_acc:99.7889\n",
      "epoch: 1 batch:1148 train_loss:0.0125 train_acc:99.7890\n",
      "epoch: 1 batch:1149 train_loss:0.0125 train_acc:99.7892\n",
      "epoch: 1 batch:1150 train_loss:0.0124 train_acc:99.7894\n",
      "epoch: 1 batch:1151 train_loss:0.0124 train_acc:99.7896\n",
      "epoch: 1 batch:1152 train_loss:0.0124 train_acc:99.7898\n",
      "epoch: 1 batch:1153 train_loss:0.0124 train_acc:99.7900\n",
      "epoch: 1 batch:1154 train_loss:0.0124 train_acc:99.7901\n",
      "epoch: 1 batch:1155 train_loss:0.0124 train_acc:99.7903\n",
      "epoch: 1 batch:1156 train_loss:0.0124 train_acc:99.7905\n",
      "epoch: 1 batch:1157 train_loss:0.0124 train_acc:99.7907\n",
      "epoch: 1 batch:1158 train_loss:0.0124 train_acc:99.7909\n",
      "epoch: 1 batch:1159 train_loss:0.0123 train_acc:99.7910\n",
      "epoch: 1 batch:1160 train_loss:0.0123 train_acc:99.7912\n",
      "epoch: 1 batch:1161 train_loss:0.0123 train_acc:99.7914\n",
      "epoch: 1 batch:1162 train_loss:0.0123 train_acc:99.7916\n",
      "epoch: 1 batch:1163 train_loss:0.0123 train_acc:99.7918\n",
      "epoch: 1 batch:1164 train_loss:0.0123 train_acc:99.7919\n",
      "epoch: 1 batch:1165 train_loss:0.0123 train_acc:99.7921\n",
      "epoch: 1 batch:1166 train_loss:0.0123 train_acc:99.7923\n",
      "epoch: 1 batch:1167 train_loss:0.0123 train_acc:99.7925\n",
      "epoch: 1 batch:1168 train_loss:0.0123 train_acc:99.7926\n",
      "epoch: 1 batch:1169 train_loss:0.0122 train_acc:99.7928\n",
      "epoch: 1 batch:1170 train_loss:0.0122 train_acc:99.7930\n",
      "epoch: 1 batch:1171 train_loss:0.0122 train_acc:99.7932\n",
      "epoch: 1 batch:1172 train_loss:0.0122 train_acc:99.7934\n",
      "epoch: 1 batch:1173 train_loss:0.0122 train_acc:99.7935\n",
      "epoch: 1 batch:1174 train_loss:0.0122 train_acc:99.7937\n",
      "epoch: 1 batch:1175 train_loss:0.0122 train_acc:99.7939\n",
      "epoch: 1 batch:1176 train_loss:0.0122 train_acc:99.7941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch:1177 train_loss:0.0122 train_acc:99.7942\n",
      "epoch: 1 batch:1178 train_loss:0.0122 train_acc:99.7944\n",
      "epoch: 1 batch:1179 train_loss:0.0121 train_acc:99.7946\n",
      "epoch: 1 batch:1180 train_loss:0.0121 train_acc:99.7948\n",
      "epoch: 1 batch:1181 train_loss:0.0121 train_acc:99.7949\n",
      "epoch: 1 batch:1182 train_loss:0.0121 train_acc:99.7951\n",
      "epoch: 1 batch:1183 train_loss:0.0121 train_acc:99.7953\n",
      "epoch: 1 batch:1184 train_loss:0.0121 train_acc:99.7954\n",
      "epoch: 1 batch:1185 train_loss:0.0121 train_acc:99.7956\n",
      "epoch: 1 batch:1186 train_loss:0.0121 train_acc:99.7958\n",
      "epoch: 1 batch:1187 train_loss:0.0121 train_acc:99.7960\n",
      "epoch: 1 batch:1188 train_loss:0.0120 train_acc:99.7961\n",
      "epoch: 1 batch:1189 train_loss:0.0120 train_acc:99.7963\n",
      "epoch: 1 batch:1190 train_loss:0.0120 train_acc:99.7965\n",
      "epoch: 1 batch:1191 train_loss:0.0120 train_acc:99.7967\n",
      "epoch: 1 batch:1192 train_loss:0.0120 train_acc:99.7968\n",
      "epoch: 1 batch:1193 train_loss:0.0120 train_acc:99.7970\n",
      "epoch: 1 batch:1194 train_loss:0.0120 train_acc:99.7972\n",
      "epoch: 1 batch:1195 train_loss:0.0120 train_acc:99.7973\n",
      "epoch: 1 batch:1196 train_loss:0.0120 train_acc:99.7975\n",
      "epoch: 1 batch:1197 train_loss:0.0120 train_acc:99.7977\n",
      "epoch: 1 batch:1198 train_loss:0.0120 train_acc:99.7978\n",
      "epoch: 1 batch:1199 train_loss:0.0119 train_acc:99.7980\n",
      "epoch: 1 batch:1200 train_loss:0.0119 train_acc:99.7982\n",
      "epoch: 1 batch:1201 train_loss:0.0119 train_acc:99.7983\n",
      "epoch: 1 batch:1202 train_loss:0.0119 train_acc:99.7985\n",
      "epoch: 1 batch:1203 train_loss:0.0119 train_acc:99.7987\n",
      "epoch: 1 batch:1204 train_loss:0.0119 train_acc:99.7988\n",
      "epoch: 1 batch:1205 train_loss:0.0119 train_acc:99.7990\n",
      "epoch: 1 batch:1206 train_loss:0.0119 train_acc:99.7992\n",
      "epoch: 1 batch:1207 train_loss:0.0119 train_acc:99.7993\n",
      "epoch: 1 batch:1208 train_loss:0.0119 train_acc:99.7995\n",
      "epoch: 1 batch:1209 train_loss:0.0118 train_acc:99.7997\n",
      "epoch: 1 batch:1210 train_loss:0.0118 train_acc:99.7998\n",
      "epoch: 1 batch:1211 train_loss:0.0118 train_acc:99.8000\n",
      "epoch: 1 batch:1212 train_loss:0.0118 train_acc:99.8002\n",
      "epoch: 1 batch:1213 train_loss:0.0118 train_acc:99.8003\n",
      "epoch: 1 batch:1214 train_loss:0.0118 train_acc:99.8005\n",
      "epoch: 1 batch:1215 train_loss:0.0118 train_acc:99.8007\n",
      "epoch: 1 batch:1216 train_loss:0.0118 train_acc:99.8008\n",
      "epoch: 1 batch:1217 train_loss:0.0118 train_acc:99.8010\n",
      "epoch: 1 batch:1218 train_loss:0.0118 train_acc:99.8012\n",
      "epoch: 1 batch:1219 train_loss:0.0117 train_acc:99.8013\n",
      "epoch: 1 batch:1220 train_loss:0.0117 train_acc:99.8015\n",
      "epoch: 1 batch:1221 train_loss:0.0117 train_acc:99.8016\n",
      "epoch: 1 batch:1222 train_loss:0.0117 train_acc:99.8018\n",
      "epoch: 1 batch:1223 train_loss:0.0117 train_acc:99.8020\n",
      "epoch: 1 batch:1224 train_loss:0.0117 train_acc:99.8021\n",
      "epoch: 1 batch:1225 train_loss:0.0117 train_acc:99.8023\n",
      "epoch: 1 batch:1226 train_loss:0.0117 train_acc:99.8025\n",
      "epoch: 1 batch:1227 train_loss:0.0117 train_acc:99.8026\n",
      "epoch: 1 batch:1228 train_loss:0.0117 train_acc:99.8028\n",
      "epoch: 1 batch:1229 train_loss:0.0117 train_acc:99.8029\n",
      "epoch: 1 batch:1230 train_loss:0.0116 train_acc:99.8031\n",
      "epoch: 1 batch:1231 train_loss:0.0116 train_acc:99.8033\n",
      "epoch: 1 batch:1232 train_loss:0.0116 train_acc:99.8034\n",
      "epoch: 1 batch:1233 train_loss:0.0116 train_acc:99.8036\n",
      "epoch: 1 batch:1234 train_loss:0.0116 train_acc:99.8037\n",
      "epoch: 1 batch:1235 train_loss:0.0116 train_acc:99.8039\n",
      "epoch: 1 batch:1236 train_loss:0.0116 train_acc:99.8041\n",
      "epoch: 1 batch:1237 train_loss:0.0116 train_acc:99.8042\n",
      "epoch: 1 batch:1238 train_loss:0.0116 train_acc:99.8044\n",
      "epoch: 1 batch:1239 train_loss:0.0116 train_acc:99.8045\n",
      "epoch: 1 batch:1240 train_loss:0.0116 train_acc:99.8047\n",
      "epoch: 1 batch:1241 train_loss:0.0115 train_acc:99.8048\n",
      "epoch: 1 batch:1242 train_loss:0.0115 train_acc:99.8050\n",
      "epoch: 1 batch:1243 train_loss:0.0115 train_acc:99.8052\n",
      "epoch: 1 batch:1244 train_loss:0.0115 train_acc:99.8053\n",
      "epoch: 1 batch:1245 train_loss:0.0115 train_acc:99.8055\n",
      "epoch: 1 batch:1246 train_loss:0.0115 train_acc:99.8056\n",
      "epoch: 1 batch:1247 train_loss:0.0115 train_acc:99.8058\n",
      "epoch: 1 batch:1248 train_loss:0.0115 train_acc:99.8059\n",
      "epoch: 1 batch:1249 train_loss:0.0115 train_acc:99.8061\n",
      "epoch: 1 batch:1250 train_loss:0.0115 train_acc:99.8063\n",
      "epoch: 1 batch:1251 train_loss:0.0115 train_acc:99.8064\n",
      "epoch: 1 batch:1252 train_loss:0.0114 train_acc:99.8066\n",
      "epoch: 1 batch:1253 train_loss:0.0114 train_acc:99.8067\n",
      "epoch: 1 batch:1254 train_loss:0.0114 train_acc:99.8069\n",
      "epoch: 1 batch:1255 train_loss:0.0114 train_acc:99.8070\n",
      "epoch: 1 batch:1256 train_loss:0.0114 train_acc:99.8072\n",
      "epoch: 1 batch:1257 train_loss:0.0114 train_acc:99.8073\n",
      "epoch: 1 batch:1258 train_loss:0.0114 train_acc:99.8075\n",
      "epoch: 1 batch:1259 train_loss:0.0114 train_acc:99.8076\n",
      "epoch: 1 batch:1260 train_loss:0.0114 train_acc:99.8078\n",
      "epoch: 1 batch:1261 train_loss:0.0114 train_acc:99.8079\n",
      "epoch: 1 batch:1262 train_loss:0.0114 train_acc:99.8081\n",
      "epoch: 1 batch:1263 train_loss:0.0113 train_acc:99.8082\n",
      "epoch: 1 batch:1264 train_loss:0.0113 train_acc:99.8084\n",
      "epoch: 1 batch:1265 train_loss:0.0113 train_acc:99.8085\n",
      "epoch: 1 batch:1266 train_loss:0.0113 train_acc:99.8087\n",
      "epoch: 1 batch:1267 train_loss:0.0113 train_acc:99.8088\n",
      "epoch: 1 batch:1268 train_loss:0.0113 train_acc:99.8090\n",
      "epoch: 1 batch:1269 train_loss:0.0113 train_acc:99.8092\n",
      "epoch: 1 batch:1270 train_loss:0.0113 train_acc:99.8093\n",
      "epoch: 1 batch:1271 train_loss:0.0113 train_acc:99.8095\n",
      "epoch: 1 batch:1272 train_loss:0.0113 train_acc:99.8096\n",
      "epoch: 1 batch:1273 train_loss:0.0113 train_acc:99.8098\n",
      "epoch: 1 batch:1274 train_loss:0.0112 train_acc:99.8099\n",
      "epoch: 1 batch:1275 train_loss:0.0112 train_acc:99.8100\n",
      "epoch: 1 batch:1276 train_loss:0.0112 train_acc:99.8102\n",
      "epoch: 1 batch:1277 train_loss:0.0112 train_acc:99.8103\n",
      "epoch: 1 batch:1278 train_loss:0.0112 train_acc:99.8105\n",
      "epoch: 1 batch:1279 train_loss:0.0112 train_acc:99.8106\n",
      "epoch: 1 batch:1280 train_loss:0.0112 train_acc:99.8108\n",
      "epoch: 1 batch:1281 train_loss:0.0112 train_acc:99.8109\n",
      "epoch: 1 batch:1282 train_loss:0.0112 train_acc:99.8111\n",
      "epoch: 1 batch:1283 train_loss:0.0112 train_acc:99.8112\n",
      "epoch: 1 batch:1284 train_loss:0.0112 train_acc:99.8114\n",
      "epoch: 1 batch:1285 train_loss:0.0112 train_acc:99.8115\n",
      "epoch: 1 batch:1286 train_loss:0.0111 train_acc:99.8117\n",
      "epoch: 1 batch:1287 train_loss:0.0111 train_acc:99.8118\n",
      "epoch: 1 batch:1288 train_loss:0.0111 train_acc:99.8120\n",
      "epoch: 1 batch:1289 train_loss:0.0111 train_acc:99.8121\n",
      "epoch: 1 batch:1290 train_loss:0.0111 train_acc:99.8123\n",
      "epoch: 1 batch:1291 train_loss:0.0111 train_acc:99.8124\n",
      "epoch: 1 batch:1292 train_loss:0.0111 train_acc:99.8125\n",
      "epoch: 1 batch:1293 train_loss:0.0111 train_acc:99.8127\n",
      "epoch: 1 batch:1294 train_loss:0.0111 train_acc:99.8128\n",
      "epoch: 1 batch:1295 train_loss:0.0111 train_acc:99.8130\n",
      "epoch: 1 batch:1296 train_loss:0.0111 train_acc:99.8131\n",
      "epoch: 1 batch:1297 train_loss:0.0110 train_acc:99.8133\n",
      "epoch: 1 batch:1298 train_loss:0.0110 train_acc:99.8134\n",
      "epoch: 1 batch:1299 train_loss:0.0110 train_acc:99.8136\n",
      "epoch: 1 batch:1300 train_loss:0.0110 train_acc:99.8137\n",
      "epoch: 1 batch:1301 train_loss:0.0110 train_acc:99.8138\n",
      "epoch: 1 batch:1302 train_loss:0.0110 train_acc:99.8140\n",
      "epoch: 1 batch:1303 train_loss:0.0110 train_acc:99.8141\n",
      "epoch: 1 batch:1304 train_loss:0.0110 train_acc:99.8143\n",
      "epoch: 1 batch:1305 train_loss:0.0110 train_acc:99.8144\n",
      "epoch: 1 batch:1306 train_loss:0.0110 train_acc:99.8146\n",
      "epoch: 1 batch:1307 train_loss:0.0110 train_acc:99.8147\n",
      "epoch: 1 batch:1308 train_loss:0.0110 train_acc:99.8148\n",
      "epoch: 1 batch:1309 train_loss:0.0109 train_acc:99.8150\n",
      "epoch: 1 batch:1310 train_loss:0.0109 train_acc:99.8151\n",
      "epoch: 1 batch:1311 train_loss:0.0109 train_acc:99.8153\n",
      "epoch: 1 batch:1312 train_loss:0.0109 train_acc:99.8154\n",
      "epoch: 1 batch:1313 train_loss:0.0109 train_acc:99.8155\n",
      "epoch: 1 batch:1314 train_loss:0.0109 train_acc:99.8157\n",
      "epoch: 1 batch:1315 train_loss:0.0109 train_acc:99.8158\n",
      "epoch: 1 batch:1316 train_loss:0.0109 train_acc:99.8160\n",
      "epoch: 1 batch:1317 train_loss:0.0109 train_acc:99.8161\n",
      "epoch: 1 batch:1318 train_loss:0.0109 train_acc:99.8162\n",
      "epoch: 1 batch:1319 train_loss:0.0109 train_acc:99.8164\n",
      "epoch: 1 batch:1320 train_loss:0.0109 train_acc:99.8165\n",
      "epoch: 1 batch:1321 train_loss:0.0109 train_acc:99.8167\n",
      "epoch: 1 batch:1322 train_loss:0.0108 train_acc:99.8168\n",
      "epoch: 1 batch:1323 train_loss:0.0108 train_acc:99.8169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch:1324 train_loss:0.0108 train_acc:99.8171\n",
      "epoch: 1 batch:1325 train_loss:0.0108 train_acc:99.8172\n",
      "epoch: 1 batch:1326 train_loss:0.0108 train_acc:99.8174\n",
      "epoch: 1 batch:1327 train_loss:0.0108 train_acc:99.8175\n",
      "epoch: 1 batch:1328 train_loss:0.0108 train_acc:99.8176\n",
      "epoch: 1 batch:1329 train_loss:0.0108 train_acc:99.8178\n",
      "epoch: 1 batch:1330 train_loss:0.0108 train_acc:99.8179\n",
      "epoch: 1 batch:1331 train_loss:0.0108 train_acc:99.8180\n",
      "epoch: 1 batch:1332 train_loss:0.0108 train_acc:99.8182\n",
      "epoch: 1 batch:1333 train_loss:0.0108 train_acc:99.8183\n",
      "epoch: 1 batch:1334 train_loss:0.0107 train_acc:99.8185\n",
      "epoch: 1 batch:1335 train_loss:0.0107 train_acc:99.8186\n",
      "epoch: 1 batch:1336 train_loss:0.0107 train_acc:99.8187\n",
      "epoch: 1 batch:1337 train_loss:0.0107 train_acc:99.8189\n",
      "epoch: 1 batch:1338 train_loss:0.0107 train_acc:99.8190\n",
      "epoch: 1 batch:1339 train_loss:0.0107 train_acc:99.8191\n",
      "epoch: 1 batch:1340 train_loss:0.0107 train_acc:99.8193\n",
      "epoch: 1 batch:1341 train_loss:0.0107 train_acc:99.8194\n",
      "epoch: 1 batch:1342 train_loss:0.0107 train_acc:99.8195\n",
      "epoch: 1 batch:1343 train_loss:0.0107 train_acc:99.8197\n",
      "epoch: 1 batch:1344 train_loss:0.0107 train_acc:99.8198\n",
      "epoch: 1 batch:1345 train_loss:0.0107 train_acc:99.8199\n",
      "epoch: 1 batch:1346 train_loss:0.0107 train_acc:99.8201\n",
      "epoch: 1 batch:1347 train_loss:0.0106 train_acc:99.8202\n",
      "epoch: 1 batch:1348 train_loss:0.0106 train_acc:99.8203\n",
      "epoch: 1 batch:1349 train_loss:0.0106 train_acc:99.8205\n",
      "epoch: 1 batch:1350 train_loss:0.0106 train_acc:99.8206\n",
      "epoch: 1 batch:1351 train_loss:0.0106 train_acc:99.8207\n",
      "epoch: 1 batch:1352 train_loss:0.0106 train_acc:99.8209\n",
      "epoch: 1 batch:1353 train_loss:0.0106 train_acc:99.8210\n",
      "epoch: 1 batch:1354 train_loss:0.0106 train_acc:99.8211\n",
      "epoch: 1 batch:1355 train_loss:0.0106 train_acc:99.8213\n",
      "epoch: 1 batch:1356 train_loss:0.0106 train_acc:99.8214\n",
      "epoch: 1 batch:1357 train_loss:0.0106 train_acc:99.8215\n",
      "epoch: 1 batch:1358 train_loss:0.0106 train_acc:99.8217\n",
      "epoch: 1 batch:1359 train_loss:0.0106 train_acc:99.8218\n",
      "epoch: 1 batch:1360 train_loss:0.0105 train_acc:99.8219\n",
      "epoch: 1 batch:   1 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:   2 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:   3 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:   4 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:   5 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:   6 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:   7 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:   8 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:   9 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  10 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  11 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  12 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  13 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  14 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  15 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  16 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  17 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  18 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  19 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  20 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  21 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  22 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  23 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  24 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  25 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  26 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  27 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  28 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  29 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  30 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  31 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  32 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  33 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  34 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  35 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  36 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  37 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  38 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  39 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  40 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  41 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  42 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  43 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  44 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  45 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  46 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  47 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  48 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  49 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  50 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  51 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  52 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  53 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  54 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  55 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  56 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  57 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  58 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  59 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  60 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  61 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  62 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  63 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  64 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  65 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  66 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  67 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  68 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  69 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  70 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  71 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  72 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  73 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  74 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  75 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  76 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  77 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  78 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  79 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  80 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  81 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  82 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  83 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  84 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  85 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  86 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  87 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  88 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  89 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  90 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  91 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  92 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  93 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  94 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  95 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  96 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  97 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  98 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch:  99 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 100 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 101 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 102 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 103 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 104 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 105 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 106 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 107 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 108 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 109 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 110 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 111 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 112 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 113 test_loss:0.0001 test_acc:100.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 114 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 115 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 116 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 117 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 118 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 119 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 120 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 121 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 122 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 123 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 124 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 125 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 126 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 127 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 128 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 129 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 130 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 131 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 132 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 133 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 134 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 135 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 136 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 137 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 138 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 139 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 140 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 141 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 142 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 143 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 144 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 145 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 146 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 147 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 148 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 149 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 150 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 151 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 152 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 153 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 154 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 155 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 156 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 157 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 158 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 159 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 160 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 161 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 162 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 163 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 164 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 165 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 166 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 167 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 168 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 169 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 170 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 171 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 172 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 173 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 174 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 175 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 176 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 177 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 178 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 179 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 180 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 181 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 182 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 183 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 184 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 185 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 186 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 187 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 188 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 189 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 190 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 191 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 192 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 193 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 194 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 195 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 196 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 197 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 198 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 199 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 200 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 201 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 202 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 203 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 204 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 205 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 206 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 207 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 208 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 209 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 210 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 211 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 212 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 213 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 214 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 215 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 216 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 217 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 218 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 219 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 220 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 221 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 222 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 223 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 224 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 225 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 226 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 227 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 228 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 229 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 230 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 231 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 232 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 233 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 234 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 235 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 236 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 237 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 238 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 239 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 240 test_loss:0.0001 test_acc:100.0000\n",
      "epoch: 1 batch: 241 test_loss:0.0006 test_acc:99.9935\n",
      "epoch: 1 batch: 242 test_loss:0.0006 test_acc:99.9935\n",
      "epoch: 1 batch: 243 test_loss:0.0006 test_acc:99.9936\n",
      "epoch: 1 batch: 244 test_loss:0.0006 test_acc:99.9936\n",
      "epoch: 1 batch: 245 test_loss:0.0006 test_acc:99.9936\n",
      "epoch: 1 batch: 246 test_loss:0.0006 test_acc:99.9936\n",
      "epoch: 1 batch: 247 test_loss:0.0006 test_acc:99.9937\n",
      "epoch: 1 batch: 248 test_loss:0.0006 test_acc:99.9937\n",
      "epoch: 1 batch: 249 test_loss:0.0006 test_acc:99.9937\n",
      "epoch: 1 batch: 250 test_loss:0.0006 test_acc:99.9938\n",
      "epoch: 1 batch: 251 test_loss:0.0006 test_acc:99.9938\n",
      "epoch: 1 batch: 252 test_loss:0.0006 test_acc:99.9938\n",
      "epoch: 1 batch: 253 test_loss:0.0005 test_acc:99.9938\n",
      "epoch: 1 batch: 254 test_loss:0.0005 test_acc:99.9938\n",
      "epoch: 1 batch: 255 test_loss:0.0005 test_acc:99.9939\n",
      "epoch: 1 batch: 256 test_loss:0.0005 test_acc:99.9939\n",
      "epoch: 1 batch: 257 test_loss:0.0005 test_acc:99.9939\n",
      "epoch: 1 batch: 258 test_loss:0.0005 test_acc:99.9939\n",
      "epoch: 1 batch: 259 test_loss:0.0005 test_acc:99.9940\n",
      "epoch: 1 batch: 260 test_loss:0.0005 test_acc:99.9940\n",
      "epoch: 1 batch: 261 test_loss:0.0005 test_acc:99.9940\n",
      "epoch: 1 batch: 262 test_loss:0.0005 test_acc:99.9940\n",
      "epoch: 1 batch: 263 test_loss:0.0005 test_acc:99.9941\n",
      "epoch: 1 batch: 264 test_loss:0.0005 test_acc:99.9941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 265 test_loss:0.0005 test_acc:99.9941\n",
      "epoch: 1 batch: 266 test_loss:0.0005 test_acc:99.9941\n",
      "epoch: 1 batch: 267 test_loss:0.0005 test_acc:99.9941\n",
      "epoch: 1 batch: 268 test_loss:0.0005 test_acc:99.9942\n",
      "epoch: 1 batch: 269 test_loss:0.0005 test_acc:99.9942\n",
      "epoch: 1 batch: 270 test_loss:0.0005 test_acc:99.9942\n",
      "epoch: 1 batch: 271 test_loss:0.0005 test_acc:99.9942\n",
      "epoch: 1 batch: 272 test_loss:0.0005 test_acc:99.9943\n",
      "epoch: 1 batch: 273 test_loss:0.0005 test_acc:99.9943\n",
      "epoch: 1 batch: 274 test_loss:0.0005 test_acc:99.9943\n",
      "epoch: 1 batch: 275 test_loss:0.0005 test_acc:99.9943\n",
      "epoch: 1 batch: 276 test_loss:0.0005 test_acc:99.9943\n",
      "epoch: 1 batch: 277 test_loss:0.0005 test_acc:99.9944\n",
      "epoch: 1 batch: 278 test_loss:0.0005 test_acc:99.9944\n",
      "epoch: 1 batch: 279 test_loss:0.0005 test_acc:99.9944\n",
      "epoch: 1 batch: 280 test_loss:0.0005 test_acc:99.9944\n",
      "epoch: 1 batch: 281 test_loss:0.0005 test_acc:99.9944\n",
      "epoch: 1 batch: 282 test_loss:0.0005 test_acc:99.9945\n",
      "epoch: 1 batch: 283 test_loss:0.0005 test_acc:99.9945\n",
      "epoch: 1 batch: 284 test_loss:0.0005 test_acc:99.9945\n",
      "epoch: 1 batch: 285 test_loss:0.0005 test_acc:99.9945\n",
      "epoch: 1 batch: 286 test_loss:0.0005 test_acc:99.9945\n",
      "epoch: 1 batch: 287 test_loss:0.0005 test_acc:99.9946\n",
      "epoch: 1 batch: 288 test_loss:0.0005 test_acc:99.9946\n",
      "epoch: 1 batch: 289 test_loss:0.0005 test_acc:99.9946\n",
      "epoch: 1 batch: 290 test_loss:0.0005 test_acc:99.9946\n",
      "epoch: 1 batch: 291 test_loss:0.0005 test_acc:99.9946\n",
      "epoch: 1 batch: 292 test_loss:0.0005 test_acc:99.9946\n",
      "epoch: 1 batch: 293 test_loss:0.0005 test_acc:99.9947\n",
      "epoch: 1 batch: 294 test_loss:0.0005 test_acc:99.9947\n",
      "epoch: 1 batch: 295 test_loss:0.0005 test_acc:99.9947\n",
      "epoch: 1 batch: 296 test_loss:0.0005 test_acc:99.9947\n",
      "epoch: 1 batch: 297 test_loss:0.0005 test_acc:99.9947\n",
      "epoch: 1 batch: 298 test_loss:0.0005 test_acc:99.9948\n",
      "epoch: 1 batch: 299 test_loss:0.0005 test_acc:99.9948\n",
      "epoch: 1 batch: 300 test_loss:0.0005 test_acc:99.9948\n",
      "epoch: 1 batch: 301 test_loss:0.0005 test_acc:99.9948\n",
      "epoch: 1 batch: 302 test_loss:0.0005 test_acc:99.9948\n",
      "epoch: 1 batch: 303 test_loss:0.0005 test_acc:99.9948\n",
      "epoch: 1 batch: 304 test_loss:0.0005 test_acc:99.9949\n",
      "epoch: 1 batch: 305 test_loss:0.0005 test_acc:99.9949\n",
      "epoch: 1 batch: 306 test_loss:0.0005 test_acc:99.9949\n",
      "epoch: 1 batch: 307 test_loss:0.0005 test_acc:99.9949\n",
      "epoch: 1 batch: 308 test_loss:0.0005 test_acc:99.9949\n",
      "epoch: 1 batch: 309 test_loss:0.0005 test_acc:99.9949\n",
      "epoch: 1 batch: 310 test_loss:0.0005 test_acc:99.9950\n",
      "epoch: 1 batch: 311 test_loss:0.0005 test_acc:99.9950\n",
      "epoch: 1 batch: 312 test_loss:0.0005 test_acc:99.9950\n",
      "epoch: 1 batch: 313 test_loss:0.0005 test_acc:99.9950\n",
      "epoch: 1 batch: 314 test_loss:0.0005 test_acc:99.9950\n",
      "epoch: 1 batch: 315 test_loss:0.0005 test_acc:99.9950\n",
      "epoch: 1 batch: 316 test_loss:0.0005 test_acc:99.9951\n",
      "epoch: 1 batch: 317 test_loss:0.0005 test_acc:99.9951\n",
      "epoch: 1 batch: 318 test_loss:0.0005 test_acc:99.9951\n",
      "epoch: 1 batch: 319 test_loss:0.0005 test_acc:99.9951\n",
      "epoch: 1 batch: 320 test_loss:0.0004 test_acc:99.9951\n",
      "epoch: 1 batch: 321 test_loss:0.0004 test_acc:99.9951\n",
      "epoch: 1 batch: 322 test_loss:0.0004 test_acc:99.9951\n",
      "epoch: 1 batch: 323 test_loss:0.0004 test_acc:99.9952\n",
      "epoch: 1 batch: 324 test_loss:0.0004 test_acc:99.9952\n",
      "epoch: 1 batch: 325 test_loss:0.0004 test_acc:99.9952\n",
      "epoch: 1 batch: 326 test_loss:0.0004 test_acc:99.9952\n",
      "epoch: 1 batch: 327 test_loss:0.0004 test_acc:99.9952\n",
      "epoch: 1 batch: 328 test_loss:0.0004 test_acc:99.9952\n",
      "epoch: 1 batch: 329 test_loss:0.0004 test_acc:99.9953\n",
      "epoch: 1 batch: 330 test_loss:0.0004 test_acc:99.9953\n",
      "epoch: 1 batch: 331 test_loss:0.0004 test_acc:99.9953\n",
      "epoch: 1 batch: 332 test_loss:0.0004 test_acc:99.9953\n",
      "epoch: 1 batch: 333 test_loss:0.0004 test_acc:99.9953\n",
      "epoch: 1 batch: 334 test_loss:0.0004 test_acc:99.9953\n",
      "epoch: 1 batch: 335 test_loss:0.0004 test_acc:99.9953\n",
      "epoch: 1 batch: 336 test_loss:0.0004 test_acc:99.9953\n",
      "epoch: 1 batch: 337 test_loss:0.0004 test_acc:99.9954\n",
      "epoch: 1 batch: 338 test_loss:0.0004 test_acc:99.9954\n",
      "epoch: 1 batch: 339 test_loss:0.0004 test_acc:99.9954\n",
      "epoch: 1 batch: 340 test_loss:0.0004 test_acc:99.9954\n"
     ]
    }
   ],
   "source": [
    "model.zero_grad()\n",
    "for epoch in range(epochs):\n",
    "    running_loss_val = 0.0\n",
    "    running_acc = 0.0\n",
    "    \n",
    "    for batch_index, batch_dict in enumerate(train_dataloader):\n",
    "        model.train()\n",
    "        batch_dict = tuple(t.to(device) for t in batch_dict)\n",
    "        outputs = model(\n",
    "            batch_dict[0],\n",
    "            # attention_mask=batch_dict[1],\n",
    "            labels = batch_dict[3]\n",
    "        )\n",
    "            \n",
    "        loss, logits = outputs[:2]\n",
    "        loss.sum().backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()  # Update learning rate schedule\n",
    "        model.zero_grad()\n",
    "            \n",
    "        # compute the loss\n",
    "        loss_t = loss.item()\n",
    "        running_loss_val += (loss_t - running_loss_val) / (batch_index + 1)\n",
    "\n",
    "        # compute the accuracy\n",
    "        acc_t = compute_accuracy(logits, batch_dict[3])\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "        # log\n",
    "        print(\"epoch:%2d batch:%4d train_loss:%2.4f train_acc:%3.4f\"%(epoch+1, batch_index+1, running_loss_val, running_acc))\n",
    "        \n",
    "    running_loss_val = 0.0\n",
    "    running_acc = 0.0\n",
    "    for batch_index, batch_dict in enumerate(test_dataloader):\n",
    "        model.eval()\n",
    "        batch_dict = tuple(t.to(device) for t in batch_dict)\n",
    "        outputs = model(\n",
    "            batch_dict[0],\n",
    "            # attention_mask=batch_dict[1],\n",
    "            labels = batch_dict[3]\n",
    "        )\n",
    "        loss,logits = outputs[:2]\n",
    "            \n",
    "        # compute the loss\n",
    "        loss_t = loss.item()\n",
    "        running_loss_val += (loss_t - running_loss_val) / (batch_index + 1)\n",
    "\n",
    "        # compute the accuracy\n",
    "        acc_t = compute_accuracy(logits, batch_dict[3])\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "        # log\n",
    "        print(\"epoch:%2d batch:%4d test_loss:%2.4f test_acc:%3.4f\"%(epoch+1, batch_index+1, running_loss_val, running_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "confidential-generic",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained('trained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "accredited-anger",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model_classification_gpu_epoch_\"+str(epochs)+\"_batch_\"+str(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-color",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-alloy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "apparent-rainbow",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file = open('trained_model/data_features.pkl', 'rb')\n",
    "data_features = pickle.load(pkl_file)\n",
    "answer_dic = data_features['answer_dic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "thermal-messenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALBERT\n",
    "# model_setting = {\n",
    "#     \"model_name\":\"albert\", \n",
    "#     \"config_file_path\":\"trained_model/config.json\", \n",
    "#     \"model_file_path\":\"trained_model/pytorch_model.bin\", \n",
    "#     \"vocab_file_path\":\"albert/albert_tiny/vocab.txt\",\n",
    "#     \"num_labels\":2 # 分幾類\n",
    "# }\n",
    "model_setting = {\n",
    "    \"model_name\":\"bert\", \n",
    "    \"config_file_path\":\"bert-base-chinese\", \n",
    "    \"model_file_path\":\"bert-base-chinese\", \n",
    "    \"vocab_file_path\":\"bert-base-chinese-vocab.txt\",\n",
    "    \"num_labels\":2  # 分幾類 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "instant-poultry",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = use_model(**model_setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fitting-gregory",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_inputs = [\n",
    "    '為何路邊停車格有編號的要收費，無編號的不用收費',\n",
    "    '債權人可否向稅捐稽徵處申請查調債務人之財產、所得資料',\n",
    "    '想做大腸癌檢測，不知道轉到哪一個辦事處',\n",
    "    'Bruce要轉錢給Jack',\n",
    "    '我想轉帳1000元給老師',\n",
    "    '轉帳給父親的戶頭從台幣戶3137元'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fancy-broadcast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2999, -0.5514]], grad_fn=<AddmmBackward>)\n",
      "為何路邊停車格有編號的要收費，無編號的不用收費\n",
      "Action: OTHER\n",
      "\n",
      "tensor([[ 0.1944, -0.5152]], grad_fn=<AddmmBackward>)\n",
      "債權人可否向稅捐稽徵處申請查調債務人之財產、所得資料\n",
      "Action: OTHER\n",
      "\n",
      "tensor([[-0.0056, -0.6463]], grad_fn=<AddmmBackward>)\n",
      "想做大腸癌檢測，不知道轉到哪一個辦事處\n",
      "Action: OTHER\n",
      "\n",
      "tensor([[-0.1361, -0.3262]], grad_fn=<AddmmBackward>)\n",
      "Bruce要轉錢給Jack\n",
      "Action: OTHER\n",
      "\n",
      "tensor([[-0.1290, -0.3830]], grad_fn=<AddmmBackward>)\n",
      "我想轉帳1000元給老師\n",
      "Action: OTHER\n",
      "\n",
      "tensor([[-0.3360, -0.6764]], grad_fn=<AddmmBackward>)\n",
      "轉帳給父親的戶頭從台幣戶3137元\n",
      "Action: OTHER\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for q_input in q_inputs:\n",
    "    bert_ids = to_bert_ids(tokenizer, q_input)\n",
    "    assert len(bert_ids) <= 512\n",
    "    input_ids = torch.LongTensor(bert_ids).unsqueeze(0)\n",
    "\n",
    "    # predict\n",
    "    outputs = model(input_ids)\n",
    "    predicts = outputs[:2]\n",
    "    predicts = predicts[0]\n",
    "    print(predicts)\n",
    "    max_val = torch.max(predicts)\n",
    "    label = (predicts == max_val).nonzero().numpy()[0][1]\n",
    "    ans_label = answer_dic.to_text(label)\n",
    "        \n",
    "    print(q_input)\n",
    "    print(\"Action: \" + ans_label)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-amendment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-wisdom",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
