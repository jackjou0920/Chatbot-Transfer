{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hungry-sound",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "animated-signal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(n_gpu, torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pending-medication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model time: 3.2227115631103516\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = torch.load(\"model_ner_gpu_epoch_2_batch_64\")\n",
    "print (\"load model time:\", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "motivated-tennis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TransFr_B': 0,\n",
       " 'TransFr_I': 1,\n",
       " 'TransTo_B': 2,\n",
       " 'TransTo_I': 3,\n",
       " 'AMOUNT_B': 4,\n",
       " 'BANK_B': 5,\n",
       " 'BANK_I': 6,\n",
       " 'O': 7}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_vals = ['TransFr_B', 'TransFr_I', 'TransTo_B', 'TransTo_I', 'AMOUNT_B', 'BANK_B', 'BANK_I', 'O']\n",
    "tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tropical-pacific",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 30\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "negative-disco",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "permanent-standard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Test Sentence:\n",
      "['轉', '給', '我', '老', '闆', '的', '上', '海', '商', '銀', '287', '##3', '元'] 13\n",
      "--------------------------------------------------\n",
      "Padding Test Sequence:\n",
      "[ 6752  5183  2769  5439  7293  4638   677  3862  1555  7065 11525  8152\n",
      "  1039     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# test_sentence = [\"我想要給我的房東台新100元\"]\n",
    "# test_sentence = [\"我想要給我老婆100塊從我台幣帳戶\"]\n",
    "test_sentence = [\"轉給我老闆的上海商銀2873元\"]\n",
    "\n",
    "tokenized_test_texts = [tokenizer.tokenize(sent) for sent in test_sentence]\n",
    "print(\"Tokenized Test Sentence:\")\n",
    "print(tokenized_test_texts[0], len(tokenized_test_texts[0]))\n",
    "print(\"-\"*50)\n",
    "\n",
    "input_ids = pad_sequences(\n",
    "    [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_test_texts],\n",
    "    maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\"\n",
    ")\n",
    "test_attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n",
    "print(\"Padding Test Sequence:\")\n",
    "print(input_ids[0])\n",
    "print(test_attention_masks[0])\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "weekly-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = torch.tensor(input_ids)\n",
    "test_masks = torch.tensor(test_attention_masks)\n",
    "\n",
    "test_sentence_data = TensorDataset(test_inputs, test_masks)\n",
    "train_sentence_sampler = RandomSampler(test_sentence_data)\n",
    "test_sentence_dataloader = DataLoader(test_sentence_data, sampler=train_sentence_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "arctic-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "for batch in test_sentence_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tmp_eval_loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        \n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "\n",
    "pred_tags = [[tags_vals[p_i] for p_i in p] for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "musical-qatar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['轉', '給', '我', '老', '闆', '的', '上', '海', '商', '銀', '287', '##3', '元'] 13\n",
      "\n",
      "['轉', '給', '我', '老', '闆', '的', '上', '海', '商', '銀', '2873', '元'] 12\n",
      "['O', 'O', 'O', 'TransTo_B', 'TransTo_I', 'O', 'BANK_B', 'BANK_I', 'BANK_I', 'BANK_I', 'AMOUNT_B', 'O'] 12\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_test_texts[0], len(tokenized_test_texts[0]))\n",
    "print()\n",
    "token = list()\n",
    "for tok in tokenized_test_texts[0]:\n",
    "    if ('##' in tok):\n",
    "        token[-1] = token[-1] + tok.replace('##', '')\n",
    "    else:\n",
    "        token.append(tok)\n",
    "        \n",
    "token_tag = pred_tags[0][:len(token)]\n",
    "print(token, len(token))\n",
    "print(token_tag, len(token_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "blond-costume",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time: 30.33272933959961\n"
     ]
    }
   ],
   "source": [
    "print (\"predict time:\", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cloudy-burden",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransFr: []\n",
      "TransTo: ['老闆']\n",
      "BANK: ['上海商銀']\n",
      "AMOUNT: ['2873']\n"
     ]
    }
   ],
   "source": [
    "trans_to_list, trans_to = list(), \"\"\n",
    "trans_from_list, trans_from = list(), \"\"\n",
    "bank_list, bank = list(), \"\"\n",
    "amount_list = list()\n",
    "\n",
    "for i in range(len(token_tag)):\n",
    "    if (token_tag[i] == \"TransTo_B\" or token_tag[i] == \"TransTo_I\"):\n",
    "        trans_to += token[i]\n",
    "        if (i+1 >= len(token_tag) or token_tag[i+1] != \"TransTo_I\"):\n",
    "            if (trans_to != \"\"):\n",
    "                trans_to_list.append(trans_to)\n",
    "            trans_to = \"\"\n",
    "    elif (token_tag[i] == \"TransFr_B\" or token_tag[i] == \"TransFr_I\"):\n",
    "        trans_from += token[i]\n",
    "        if (i+1 >= len(token_tag) or token_tag[i+1] != \"TransFr_I\"):\n",
    "            if (trans_from != \"\"):\n",
    "                trans_from_list.append(trans_from)\n",
    "            trans_from = \"\"\n",
    "    elif (token_tag[i] == \"BANK_B\" or token_tag[i] == \"BANK_I\"):\n",
    "        bank += token[i]\n",
    "        if (i+1 >= len(token_tag) or token_tag[i+1] != \"BANK_I\"):\n",
    "            if (bank != \"\"):\n",
    "                bank_list.append(bank)\n",
    "            bank = \"\"\n",
    "    elif (token_tag[i] == \"AMOUNT_B\"):\n",
    "        amount_list.append(token[i])\n",
    "            \n",
    "print(\"TransFr:\", trans_from_list)\n",
    "print(\"TransTo:\", trans_to_list)\n",
    "print(\"BANK:\", bank_list)\n",
    "print(\"AMOUNT:\", amount_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-southeast",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-aggregate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "competitive-korea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "increasing-preservation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.431 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "jieba.load_userdict('./userdict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "familiar-reunion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['請', '幫忙', '從', '我', '的', '薪轉', '轉', '3993', '元給', '我', '的', '父親', '帳戶']\n"
     ]
    }
   ],
   "source": [
    "sentence = '請幫忙從我的薪轉轉3993元給我的父親帳戶'\n",
    "seg_list = jieba.lcut(sentence)\n",
    "print(seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-robertson",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
